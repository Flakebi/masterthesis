\clearpage
\bigsection{Implementation}
The implementation consists of several steps. First, we have to insert instrumentation instructions. This happens inside the LLVM compiler. LLVM already has the ability to emit instrumentation instructions, it only needs to be activated. The activation can be accomplished in LLPC with a few lines of code. The next step is loading the compiled binary onto the GPU, which needs to be adjusted to work. Our changes at this part are summarized in \cref{sub:loading}. After loading a shader it is run. This part is unmodified by our work. When a shader pipeline is destroyed, the driver fetches the collected information and writes them to disk. This part is explained in detail in \cref{sub:save}. The next compilation of a profiled shader can then incorporate the collected information, which is described in \cref{sub:load}.

\subsection{Workflow}
\label{sub:impl_workflow}
We use environment variables to pass options to the driver as described in \cref{sub:workflow}.
The concrete steps then are like following:
\begin{enumerate}
	\item Set \texttt{AMDVLK\_PROFILE\_INSTR\_GEN} to a profile file path
	\item Run the application $n$ times and produces an output file on each run
	\item Merge the $n$ files for each pipeline
	\item Set \texttt{AMDVLK\_PROFILE\_INSTR\_USE} to the path of the merged files and run the application again
\end{enumerate}
The paths to profile data files should contain a \texttt{\%i} which will get replaced by the pipeline id when data is loaded and stored.
When generating data, \texttt{\%m} can also be used to insert a unique id by LLVM into the filename.

By default, the basic block counters are inserted at before the CFG gets structurized and they are counting for each SIMD lane.
To count the executed basic blocks per SIMD unit instead of per lane and to insert them after structurizing the CFG, we set the environment variables \texttt{AMDVLK\_PROFILE\_PER\_WAVE=1 AMDVLK\_PROFILE\_LATE=1}.
To add profiling and usage for the uniformity of special variables, we set the environment variable \texttt{AMDVLK\_PROFILE\_UNIFORM=1}.
When using the previously collected data, we have the option to analyze the outcome and write it to a file. The variable \texttt{AMDVLK\_PROFILE\_ANALYSIS=1} is used for this purpose.

\subsection{Compiling Shaders}
\label{sub:compiling}
The compilation of shaders in LLPC needs to be adjusted slightly to also generate profiling instrumentation. In detail, we need to set the PGO options of the LLVM \texttt{PassManagerBuilder}. One option that we activate is the use of atomic operations for counters. The execution of a shader on a GPU is highly parallel, so we expect that multiple shaders will write to the same counter simultaneously. Using atomic operations ensures that no executions are lost in the counter. LLVM generates an ELF file with additional data and code sections that contain e.g. basic block counters and the registered destructor to write the data to disk. We cannot directly use the destructor generated by LLVM because code on the GPU does not have access to the file system on the host. Our solution is described in \cref{sub:save}.

\subsection{Basic Block Counters}
\label{sub:bbcounters}
To insert the PGO counters after structurizing the CFG, we do not set the options in the \texttt{PassManagerBuilder} but we insert the instrumentation passes manually at a later point in the pipeline.

When the option to count per SIMD unit is activated, we do not emit a single atomic add but the following code instead:
% TODO
\begin{lstlisting}[caption={Counting per SIMD unit},language={[x86masm]Assembler}]
savereg = exec
exec = 1
atomic_add
exec = savereg
\end{lstlisting}

\subsection{Uniform Variables}
\label{sub:impl_uniformity}
% TODO
Reuse counter infrastructure of basic block counting so the only new thing is how to find uniformity.
As with basic block counters, we can capture the uniformity of variables before or after the \texttt{StructurizeCFG} pass. If we capture before structurizing, we should count the amount of lanes that are affected by a divergent variable.
So when a SIMD unit currently has 12 active lanes and the variable of interest is not uniform across these lanes, we add 12 to the counter.
On the other hand if we insert the counters after structurization, we want to count the SIMD units instead of lanes. This means for a non-uniform variable, we add one to the counter, no matter how many lanes are active.
After the structurization pass ran, we cannot inspect the normal conditions of branches anymore because they do not represent the branches from the shader code.
% TODO amdgpu.if intrinsic?
Instead, we have to search for inserted \texttt{if} intrinsic and look at the uniformity of its argument.
In both cases, if the variable we are capturing is uniform, the counter keeps its value.

After capturing data, we can compare the uniformity counters with the basic block counters. Dividing the uniformity counter by the total executions, which is stored in the basic block counter, we get the fraction of runs where a variable is divergent.
\begin{lstlisting}[caption={Check a variable for uniformity},language={[x86masm]Assembler}]
%y = %x == read_first_lane
%z = popcount(vcc) > 0
only in first lane: atomic_add(%z)
\end{lstlisting}

\subsection{Loading Shaders}
\label{sub:loading}
In the AMDVLK Vulkan driver, PAL is responsible for loading the shader ELF files, copying them to GPU memory and running them. The previous ELF support of PAL was limited as it only extracted the \texttt{.text} and \texttt{.data} section (code and data) and uploaded them. This is insufficient for PGO instrumentation, we also need to load the additional data sections for counters and other data. As part of this work, we implemented loading all sections in the ELF which are marked with write or execution flags.

The code in the compiled object file contains references to the counters in the data section. As the addresses are not yet known at compile-time, LLVM adds relocation entries for them as described in \cref{sub:elf}. We implemented applying these relocations in PAL. The steps in PAL to load a shader are:
\begin{enumerate}
	\item Allocate memory for all sections on the GPU so we know their final addresses
	\item Copy sections to the allocated GPU memory by mapping the GPU memory to the CPU
	\item Iterate through all relocation sections and perform the relocations
\end{enumerate}
When performing the relocations, we read from the loaded ELF file on the CPU and write to the GPU mapped memory. Reading from GPU mapped memory would mean we have to wait until the data arrives through the PCIe bus, which takes longer than reading from CPU memory. On the other hand, writing is fine because we do not have to wait until data is actually written, we can just continue thus hiding the latency.

PAL's ELF parser contained a bug so it only supported \texttt{sh\_link} and \texttt{sh\_info} references to already loaded sections. This bug was fixed as part of this thesis.

%The string table for a given symbol table is available in the table's section header in \texttt{sh\_link}.

\subsection{Fetching and Storing Data}
\label{sub:save}
As described in \cref{sub:save-design}, we cannot create the data files from inside the shader. Instead, the driver has to fetch the data from GPU memory and write it to files.

Most parts of the file writing code of LLVM are inside a library. The library gets statically linked to the target when PGO instrumentation is enabled. We decided to link that library into PAL, so the driver can access the file writing code.
The needed information to write a file are the addresses and sizes of the PGO related ELF sections. In PAL we can read this information from the pipeline ELF file. We map the sections from the GPU memory to the CPU, write their addresses into global variables and call the LLVM PGO file dump function.
We explicitly specify the filename, which should contain the pipeline hash.

The metadata contain the absolute memory addresses of the counters and LLVM also dumps the absolute address of the counter section. The counter addresses are set through relocations when loading the ELF file which means they are virtual addresses on the GPU. The address of the counter section instead is a virtual address on the CPU which maps to GPU memory. To get a correct dump file, these addresses need to refer to the same address space. Therefore, before writing the data to a file, we replace the GPU virtual addresses in the metadata with CPU virtual addresses.

\subsection{Loading and Using Profile Data}
\label{sub:load}
Similar as when compiling shaders, we need to give the filename with PGO data to the LLVM \texttt{PassManagerBuilder}. It will add passes to set the basic block frequency information, which is then used by subsequent passes.

A problem that turned up when using profiling data is the \texttt{ControlHeightReduction} pass~\cite{ControlHeightReduction}.
This pass is specific to profile-guided optimizations and it duplicats some basic blocks and -- based on a condition -- executes one or the other.
For some shaders in Dota 2, it duplicated the last block which contains the \texttt{export} intrinsic call. This intrinsic is responsible for exporting a position in the vertex shader or a final color in the pixel shader.
While the \texttt{export} instruction still gets executed once per SIMD lane after the block duplication, it now gets executed multiple times per SIMD unit.
This leads to problems, because it has a meaning for the whole SIMD unit, e.g. the \texttt{done} flag tells the GPU that this is the last \texttt{export} instruction on this unit. Executing multiple \texttt{export} instructions with the \texttt{done} flag set leads to GPU hangs.
To fix this bug, we marked the \texttt{export} intrinsic as \emph{convergent} which means it shall not depend on additional control flow and modified the \texttt{ControlHeightReduction} pass so it does not duplicate blocks with convergent intrinsics.
