\clearpage
\bigsection{Implementation}
The implementation consists of several steps. First, we have to insert instrumentation instructions. This happens inside the compiler and LLVM already has the ability so it only needs to be activated. This can be accomplished in LLPC with a few lines of code. The next step is loading the compiled binary onto the GPU, which needs to be adjusted (see \cref{sub:loading}). After a shader is loaded and run, the driver fetches the collected information and writes the to disk. This part is explained in detail in \cref{sub:save}. The next compilation of a profiled shader can then incorporate the collected information, which is described in \cref{sub:load}.

\subsection{Compiling Shaders}
\label{sub:compiling}
The compilation of shaders in LLPC needs to be adjusted slightly to also generate profiling instrumentation. In detail, we need to set the PGO options of the LLVM \texttt{PassManagerBuilder}. LLVM then generates an ELF file with additional data and code sections that contain e.g. basic block counters and the registered destructor to write the data to disk. We cannot directly use the destructor generated by LLVM because code on the GPU does not have access to the file system on the host.

\subsection{Loading Shaders}
\label{sub:loading}
In the AMDVLK Vulkan driver, PAL is responsible for loading the shader ELF files, copying the to GPU memory and running them. The previous ELF support of PAL was limited as it only extracted the \texttt{.text} and \texttt{.data} section (code and data). As part of this work, we implemented loading all sections in the ELF which are marked with write or execution flags.

code contains references to counters in data section
However, compiler does not know at which address the sections will be placed in memory in the end
So the compiler inserts zeroes and encodes in the relocation sections of the ELF file that these references need to be resolved.
When compiling with PGO instrumentation on the x64\_86 architecture, the compile-time linker would resolve those relocations and create several segments (each segment contains one or more sections). For a dynamic object like a library or a PIC executable, the absolute address of the segments is not known at compile-time, however the segments have a fixed position relative to each other.
The ELF loader reads this information and loads the segments at a random offset but with a known relative position.
Relocations are only needed to resolve addresses between different dynamic objects.

In theory, this could work in the same way on GPUs. However, PAL does not support allocating memory at fixed addresses at the moment and introducing this feature would touch many parts of PAL. For this reason we decided to instead skip the compile-time linking step -- which is not done for shaders anyway at the moment -- and use the relocations to resolve all references between ELF sections.
The steps for relocations are:
\begin{enumerate}
	\item Allocate memory for all sections on the GPU so we know their final addresses
	\item Copy sections to the allocated GPU memory by mapping the GPU memory to the CPU
	\item Iterate through all relocation sections and perform the relocations
\end{enumerate}
When performing the relocations, we read from the loaded ELF file on the CPU and write to the GPU mapped memory. Reading from GPU mapped memory would mean we have to wait until the data arrives through the PCIe bus, which takes longer than reading from CPU memory. Writing on the other hand is fine because we do not have to wait until data is actually written, we can just continue thus hiding the latency.

PAL's ELF parser contained a bug so it only supported \texttt{sh\_link} and \texttt{sh\_info} references to already loaded sections. This bug was fixed as part of this thesis.

The string table for a given symbol table is available in the table's section header in \texttt{sh\_link}.

\subsection{Fetching and Storing Data}
\label{sub:save}
LLVM generates code for writing the collected data into a file and adds is as a destructor, which would be run at the end of a program. Destructors are not run for GPU code and GPUs cannot access the file system of the host. So directly creating and writing files from the GPU is impossible.

There are two possibilities: The first option is to add all needed data as metadata into the ELF file and read this from PAL and run code with the same effect.

The second possibility is the destructor and parts of the PGO instrumentation for the host architecture and embed that code in the ELF file. PAL can then load this code and just execute it.
Advantage: More general and flexible, if the code in LLVM is changed, no adjustments have to be made, it just works
Disadvantage: More complicated as we have to simultaneously compile for two different architectures and merge that into one file. Also loading the code in PAL and executing it is more sophisticated than reading metadata.

\subsection{Loading Profile Data}
\label{sub:load}