\clearpage
\bigsection{Implementation}
Our implementation happens in several steps of the PGO workflow and touches multiple parts of the AMDVLK driver.
First, the LLVM compiler inserts instrumentation instructions.
LLVM can already emit instrumentation instructions for counting basic blocks and it can be activated with a few lines of code in \acrfull{llpc}.
The next step is loading the compiled binary onto the GPU. Therefore, code in the \acrfull{pal} needs to be adjusted as summarized in \cref{sub:loading}.
After loading a shader, it is run. This part is unmodified by our work.
Then, when a shader pipeline is destroyed again, the driver fetches the collected information and writes them to disk as explained in detail in \cref{sub:save}.
The next compilation of a profiled shader can then incorporate the collected information, which is described in \cref{sub:load}. All of this gives us the possibility to use the basic block counters for PGO.

The more advanced analyses and implementations reuse this infrastructure and extend it with new LLVM passes. The implementation of these passes is described starting in \cref{sub:impl_unused_code}.

\subsection{Workflow}
\label{sub:impl_workflow}
As described in the design \cref{sub:workflow}, options are passed to the driver through environment variables.
The concrete steps are:
\begin{enumerate}
	\item Set \texttt{AMDVLK\_PROFILE\_INSTR\_GEN} and other variables to a profile file path
	\item Run the application $n$ times which produces an output file on each run
	\item Merge the $n$ files for each pipeline
	\item Set \texttt{AMDVLK\_PROFILE\_INSTR\_USE} to the path of the merged files (and more variables) and run the application again
\end{enumerate}
The paths to profile data files should contain a \texttt{\%i} which will get replaced by the pipeline id when data is loaded and stored.
When generating data, \texttt{\%m} can also be used to insert a unique id by LLVM into the filename.

The following options can be changed by environment variables:
\begin{itemize}
	\item \texttt{AMDVLK\_PROFILE\_INSTR\_GEN}: Generate instrumentation in shader code, use e.g. \texttt{Pipeline\_\%i\_\%m.profraw}
	\item \texttt{AMDVLK\_PROFILE\_INSTR\_USE}: Optimize shader code, e.g. \texttt{Pipeline\_\%i.profdata}
	\item \texttt{AMDVLK\_PROFILE\_LATE}: Add PGO passes after structurization, default is before \texttt{StructurizeCFG}
	\item \texttt{AMDVLK\_PROFILE\_PER\_WAVE}: Count executions per SIMD unit instead of per lane, only use this in combination with late instrumentation!
	\item \texttt{AMDVLK\_PROFILE\_ANALYSIS}: Add analysis passes and write results to temporary file
	\item \texttt{AMDVLK\_PROFILE\_UNIFORM}: Add passes to generate and use instrumentation for uniform variables
	\item \texttt{AMDVLK\_PROFILE\_REMOVE}: Remove basic blocks which are never executed, this has only an effect when using PGO data
	\item \texttt{AMDVLK\_PROFILE\_NON\_ATOMIC}: Use normal counters instead of atomic counters for instrumentation
\end{itemize}

The detailed effect of these variables and how they are implemented is described in the following sections.

\subsection{Compiling Shaders}
\label{sub:compiling}
This section describes our modifications of the compiler pipeline.
The changes are applied to LLPC and LLVM, which compile shaders in SPIR-V to the hardware ISA.

When the environment variable \texttt{AMDVLK\_PROFILE\_INSTR\_GEN} is set, the compiler inserts instrumentation into shaders. There, we have to distinguish between two possibilities:
In the default case, any \texttt{\%i} in the filename is replaced with the pipeline id and this filename is provided to the \texttt{PassManagerBuilder}.
Otherwise, if the variable \texttt{AMDVLK\_PROFILE\_LATE} is set, the instrumentation is inserted after the \texttt{StructurizeCFG} pass.
Therefore, the \texttt{AMDGPUTargetMachine} pass adds all needed PGO passes after structurization.

To use the most basic instrumentation with the \texttt{PassManagerBuilder}, LLPC only has to set one of its member variables to the requested filename.
To support more of our use cases, the \texttt{PassManagerBuilder} has a new member variable of the class \texttt{InstrProfOptions}.
This allows LLPC to set more options and we do not have to modify the \texttt{PassManagerBuilder}.
One of these options decides if atomic counters are used or not.
This property is set to true except if the environment variable \texttt{AMDVLK\_PROFILE\_NON\_ATOMIC} is set.
Another change in the \texttt{PassManagerBuilder} for our implementation is that our new PGO passes are added if necessary.
For example the analysis pass or the uniformity instrumentation generation pass.

When instrumenting after structurization, \texttt{AMDGPUTargetMachine} adds all needed passes.
It handles the same options as atomic counters but without the \texttt{PassManagerBuilder}, so the counter instrumentation and use passes have to be added explicitly.

The PGO passes in LLVM generate an ELF file with additional data and code sections that contain e.g. basic block counters and the registered destructor to write the collected data to disk.
We cannot directly use the destructor generated by LLVM because code on the GPU does not have access to the file system on the host, so we deactivate this code.
Our solution to save the collected data is described in \cref{sub:save}.

\subsection{Loading Shaders}
\label{sub:loading}
After LLPC and LLVM in the AMDVLK driver compiled the code, PAL is responsible for loading the shader ELF files.
This means copying the compiled code to GPU memory and running the shaders.
The previous ELF support of PAL was limited as it only extracted the \texttt{.text} and \texttt{.data} section (code and data) and uploaded them to GPU memory.
For PGO instrumentation, PAL also needs to load additional data sections for counters and metadata.
As part of this work, we implemented loading all sections in an ELF file which are marked with write or execution flags.

To increment counters, the code in the compiled object file references the counters in the data section.
As the final addresses are not yet known at compile-time, LLVM adds relocation entries for them as described in \cref{sub:elf}.
We implemented applying these relocations in PAL. The steps in PAL to load a shader are:
\begin{enumerate}
	\item Allocate memory for all sections on the GPU, so the driver knows their final addresses
	\item Copy sections to the allocated GPU memory by mapping the GPU memory to the CPU
	\item Iterate through all relocation sections and perform the relocations
\end{enumerate}
When performing the relocations, PAL reads from the loaded ELF file on the CPU and write to the GPU mapped memory.
Reading from GPU mapped memory would mean the CPU has to wait until the data arrives through the PCIe bus, which takes longer than reading from CPU memory.
On the other hand, writing is fine because the CPU does not have to wait until data is written, it can just continue, thus hiding the latency.

PAL's ELF parser contained a bug so it only supported \texttt{sh\_link} and \texttt{sh\_info} references to already loaded sections. This bug was fixed as part of this thesis.

%The string table for a given symbol table is available in the table's section header in \texttt{sh\_link}.

\subsection{Fetching and Storing Data}
\label{sub:save}
After loading and running a shader, the driver must save the counter values.
As described in \cref{sub:save-design}, the shader itself cannot create the data files.
Instead, the driver has to fetch the data from GPU memory and write it to files.

Most parts of the file writing code of LLVM are inside a library.
The library gets statically linked to the compiled program when PGO instrumentation is enabled.
This does not happen for shaders because they are never linked in the current implementation.
We decided to link that library into PAL, so the driver can access the file writing code.
The needed information to write a file are the addresses and sizes of the PGO related ELF sections.
PAL can read this information from the pipeline ELF file.

When a pipeline gets destroyed because the application does not need it anymore, PAL writes the collected counter data to disk.
However, not all games destroy pipelines in the end, some just quit.
To get the counter data for these games, we start a background thread that dumps the counter data every ten seconds.
A problem with this approach is that collected counters can be inconsistent when a shader is in use while the data is dumped.
For applications that destroy pipelines, we get accurate counters in the end, this problem only exists for programs that do not delete pipelines.
To collect counters, the driver maps the sections from the GPU memory to the CPU, writes their addresses into global variables and calls the LLVM PGO file dump function.
We explicitly specify the filename and replace any \texttt{\%i} inside the name with the current pipeline hash.

There is a single problem with counting on the GPU and dumping the data from the CPU:
The metadata contains the absolute memory addresses of the counters and LLVM also dumps the absolute address of the counter section.
The counter addresses are set through relocations when loading the ELF file which means they are virtual addresses on the \emph{GPU}.
The address of the counter section, on the other hand, is a virtual address on the \emph{CPU} which maps to GPU memory. To get a correct dump file, these addresses need to refer to the same address space.
Therefore, before writing the data to a file, PAL replaces the GPU virtual addresses in the metadata with CPU virtual addresses.

\subsection{Loading and Using Profile Data}
\label{sub:load}
Similar to compiling shaders, the LLVM \texttt{PassManagerBuilder} needs the filename with PGO data.
As before, we replace a \texttt{\%i} in the filename with the pipeline hash before handing the name to LLVM.
LLVM will then add passes to set the basic block frequency information, which is used by subsequent passes for optimizations.

A problem that turned up when using profiling data is the \texttt{ControlHeightReduction} pass~\cite{ControlHeightReduction}.
This pass is specific to profile-guided optimizations and it duplicates some basic blocks and --- based on a condition --- executes one or the other.
For some shaders in Dota 2, it duplicated the last block which contains the \texttt{export} intrinsic call.
This intrinsic is responsible for exporting a position in the vertex shader or a final color in the pixel shader.
While the \texttt{export} instruction still gets executed once per SIMD lane after the block duplication, it now gets executed multiple times per SIMD unit.
This leads to problems because it has a meaning for the whole SIMD unit, e.g. the \texttt{done} flag tells the GPU that this is the last \texttt{export} instruction on this unit.
Executing multiple \texttt{export} instructions with the \texttt{done} flag set leads to GPU hangs.
To fix this bug, we marked the \texttt{export} intrinsic as \emph{convergent} which means it shall not depend on additional control-flow and we modified the \texttt{ControlHeightReduction} pass so it does not duplicate regions that contain convergent intrinsics.

\subsection{Basic Block Counters}
\label{sub:bbcounters}
Counting executions of basic blocks is implemented in three different LLVM passes.
The pass \texttt{PGOInstrumentationGen} inserts increment intrinsics into some basic blocks.
After collecting data, \texttt{PGOInstrumentationUse} loads a \texttt{.profdata} file, reads the counters and sets the block frequency data.
Creating the counter data sections, storing the metadata and emitting the actual add instructions happens in the \texttt{InstrProfiling} pass.
This pass iterates over all counter intrinsics from the \texttt{PGOInstrumentationGen} pass and replaces them by add instructions.

These passes are already part of LLVM. We only modified the \texttt{InstrProfiling} pass to support per unit counters.
The details of this change are described in \cref{sub:impl_per_unit}.

\subsection{Unused Code}
\label{sub:impl_unused_code}
After collecting basic block counters, we can find code which is never executed.
To get this information, we could look into the captured profiling dumps and search for counters which are zero. But not every basic block has its own counter, as some counters can be computed from others.
As we do not want to miss these blocks, we wrote LLVM passes instead.
One pass which needs information about unused code is our analysis pass, which we use to collect information for diagrams in this thesis.
Another pass is responsible for removing unused basic blocks if the environment variable \texttt{AMDVLK\_PROFILE\_REMOVE} is set.
These passes run after the \texttt{PGOInstrumentationUse} pass so they have access to the block frequency information.

As described in the design section, because of inaccuracies, not every unused basic block contains zero in the counter.
Therefore, if a basic block is executed less than $< \SI{0.001}{\percent}$ of entry block executions, it is considered unused.
This does not work perfectly for unused blocks in loops, where the inaccuracy could be greater, but it seems to suffice in our case.

Before removing an unused block, all references to this block need to be removed.
If a conditional branch instruction references the block, the reference to the unused block can be removed by transforming the instruction to an unconditional branch to the second destination.
There are more complicated cases, e.g. an unconditional branch to the unused block or references in a switch.
All these references are redirected to a new basic block that only contains the unreachable intrinsic.
Then, when no more references exist, the unused block is removed.

\subsection{Uniform Branches}
\label{sub:impl_uniform_branches}
The design section for uniform branches explained that detecting the uniformity of branches is equal to testing if the branch condition is a uniform variable.
The implementation details to find out if a variable is uniform or not are explained in \cref{sub:impl_uniform_vars}.
This section explains peculiarities when applying this to conditions and branches.

As with basic block counters, the uniformity of branches can be captured before or after the \texttt{StructurizeCFG} pass or corresponding to that, per SIMD lane or per unit.
Before structurizing, we should count the number of lanes that are affected by a divergent variable.
So when a SIMD unit currently has 14 active lanes and the variable of interest is not uniform across these lanes, 14 is added to the counter.
On the other hand, after structurization we want to count the units instead of lanes.
This means for a non-uniform variable, the counter is incremented by one, no matter how many lanes are active.

The uniformity of branches is a special case when using this instrumentation after structurization.
The structurization is the pass which handles and eliminates divergent branch instructions, thus afterward, every branch is statically known to be uniform.
Hence, after the structurization pass ran, the normal conditions of branches cannot be inspected anymore because they do not represent the branches from the shader code.
Instead, we have to take a look at what happens with conditions in the \texttt{StructurizeCFG} pass.
The original conditions are used as an argument for the \texttt{amdgcn.if} intrinsic.
This intrinsic returns a boolean that signals if the whole unit can skip the branch which is then used for the branch instruction.
Therefore, if branches are instrumented before structurization, the compiler directly instruments the condition arguments of branches.
After structurization, it looks at the argument of \texttt{amdgcn.if} intrinsics instead.

To find out, if a branch is statically known to be uniform, we use the \texttt{DivergenceAnalysis} before structurization.
This analysis tells us for a condition if the variable is uniform or divergent.
In single cases, it may not work perfectly, e.g. if a function argument for the shader is not known to be uniform at this stage in the compilation pipeline but overall it is working good.
After structurization, if a block contains the \texttt{if} intrinsic, the condition is divergent.
On the other hand, if a block contains neither the \texttt{if} intrinsic, nor an \texttt{amdgcn.else} or \texttt{amdgcn.loop} intrinsic, it must be uniform.
At the moment, only ifs are instrumented after structurization, loops are ignored.

\subsection{Uniform Variables}
\label{sub:impl_uniform_vars}
To track the uniformity of variables, we use simple counters that stay zero for a uniform variable or get incremented for a divergent variable.
For the counters, we can reuse much of the code from basic block counters.
We can emit the same intrinsics as \texttt{PGOInstrumentationGen} and the counter sections and more complex instructions will be handled by the \texttt{InstrProfiling} pass.
We only have to take care that the counters for uniformity do not conflict with basic block counters.
Thus, we assign the uniform counters to a new (nonexistent) function, with \texttt{-uniform} appended to the original function name.

For example, if the compiler wants to check if a variable \texttt{\%0} is uniform, it emits the following code:
\begin{lstlisting}[caption={Check a variable for uniformity},language={[x86masm]Assembler}]
%1 = tail call i32 @llvm.amdgcn.readfirstlane(i32 %0)
; The 33 is the comparison code for non-equal
; Compare %0 != %1 and get the result for all lanes
%2 = tail call i64 @llvm.amdgcn.icmp.i32(i32 %0, i32 %1, i32 33)
%3 = icmp ne i64 %2, 0
%4 = zext i1 %3 to i64
; This intrinsic also takes metadata, like the function name
; Add %4 to the counter (0 if uniform, 1 if divergent)
call void @llvm.instrprof.increment.step(i8* getelementptr inbounds ([23 x i8], [23 x i8]* @__profn__amdgpu_vs_main_uniform, i32 0, i32 0), i64 0, i32 18, i32 0, i64 %4)
\end{lstlisting}

At the moment, we instrument conditions to find uniform branches and \texttt{LoadInstr}s.
The interesting parts of a load instruction are the address --- we want to know if we can use a scalar load --- and the loaded memory value.
This gives a coarse insight into how applications use memory loads and how diverse the content of their memory is.

The same analysis pass as for unused code creates statistics about the uniformity of variables
The \texttt{DivergenceAnalysis} tells us if a variable is statically known to be uniform.
If a variable is not statically uniform, it is dynamically uniform if the counter is zero or divergent if the counter is unequal to zero.

\subsection{Analyses}
\label{sub:impl_analysis}
Statistics are collected by a new LLVM pass called \texttt{PGOInstrumentationAnalysis}.
This pass uses the information from annotations and writes the data to a log file.
This log file is then processed by a Python script to aggregate the data for diagrams and tables.

One part of the collected statistic is the amount of unused basic blocks as described in \cref{sub:impl_unused_code}.
These statistics lead to the evaluation in \cref{sub:eval_counters}.
Other information which is collected through this pass is data about uniform variables.
An instrumented variable can be either statically uniform, dynamically uniform or divergent.
The most interesting cases are dynamically uniform variables: They are not detected by the compiler to be uniform, but at runtime, they turn out to be uniform.

There is one more part of the analysis which is not part of the LLVM pass.
For debugging, the driver can dump all used pipelines to a folder.
The dumps are in a human-readable form and contain the assembly code for the shaders and associated metadata.
We use these files to get the number of registers used by the shaders.

\subsection{Per Unit Counting}
\label{sub:impl_per_unit}
In the usual case, when simple atomic counters are used, one atomic operation per active lane is executed.
This means each active lane will start an atomic add to the same memory location at the same time.
These simultaneous atomics are quite slow and should be optimized to a single atomic operation per unit.
When instrumenting before structurization, this could be accomplished by the atomic optimizer pass.
Even if the atomic optimizer currently does not optimize the atomics for instrumentation, its purpose is to fuse atomic operations into a single operation per unit.
After structurization, however, the atomic optimizer already ran, and we need another way.
Also, counting per unit needs an atomic operation that gets executed on a single lane.

We implemented per unit counting in the \texttt{InstrProfiling} pass, which is responsible for lowering the profiling increment intrinsics.

An easy way to restrict the execution to a single lane is to set the EXEC mask to one. This means only the first lane will be active.
The code in \cref{lst:set_exec} accomplishes this with only three instructions overhead.
\begin{lstlisting}[caption={Restricting execution to a single SIMD unit},label=lst:set_exec,language={[x86masm]Assembler}]
; %1 = EXEC
%1 = call i64 @llvm.read_register.i64(metadata !1) #1
; EXEC = 1
call void @llvm.write_register.i64(metadata !1, i64 1) #1
; Atomic increment
%2 = atomicrmw add i64* getelementptr inbounds ([3 x i64], [3 x i64]* @__profc__amdgpu_ps_main, i64 0, i64 1), i64 1 monotonic
; Restore the EXEC register from %1
call void @llvm.write_register.i64(metadata !1, i64 %1) #1

attributes #1 = { convergent }
!1 = !{!"exec"}
\end{lstlisting}

Unfortunately, these instructions do not mark any control-flow changes for LLVM so they can get reordered and mess up the rest of the code.
After structurization, the code mostly stays in order, however, in some cases it still gets reordered, leading to graphical artifacts.
To get stable code for incrementing counters on one lane only, we reuse code from the atomic optimizer.
This code uses a few intrinsics to create a condition that will only be true for the first line.
Then it creates an if construct based on the condition where the if-block will only be run for a single lane.
It expects however to be run before the structurization pass, which will transform the introduced branches.
To be able to use this code in per unit counting, we adjust the emitted code in that case.
If late instrumentation is active, the \texttt{InstrProfiling} pass directly creates code using the condition modifying intrinsics from structurization.
I.e. it emulates the \texttt{StructurizeCFG} pass to run afterward.

The resulting code is more robust to LLVM reordering but also less efficient.
Also it does not count blocks in the per unit mode when the EXEC mask is zero, so if no SIMD lane is active but the scalar unit may still compute something.
It results in five instructions overhead when running before structurization and a few more when running afterward, also depending on the shader type (pixel shaders need special treatment when helper lanes are in use).
