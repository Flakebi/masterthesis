\clearpage
\bigsection{Implementation}
The implementation happens in several steps of the workflow and touches multiple parts of the driver.
First, inside the LLVM compiler, we have to insert instrumentation instructions.
LLVM already has the ability to emit instrumentation instructions for counting basic blocks and it can be activated with a few lines of code in LLPC.
The next step is loading the compiled binary onto the GPU. Therefore, code in PAL needs to be adjusted as summarized in \cref{sub:loading}.
After loading a shader, it is run. This part is unmodified by our work.
Then, when a shader pipeline is destroyed again, the driver fetches the collected information and writes them to disk as explained in detail in \cref{sub:save}.
The next compilation of a profiled shader can then incorporate the collected information, which is described in \cref{sub:load}. All of this gives us the possibility to use the basic block counters for PGO.

The more advanced analyses and implementations reuse this infrastructure and extend it with new LLVM passes. The implementation of these passes is described starting at \cref{sub:impl_unused_code}.

\subsection{Workflow}
\label{sub:impl_workflow}
As described in the design \cref{sub:workflow}, we use environment variables to pass options to the driver.
The concrete steps then are like following:
\begin{enumerate}
	\item Set \texttt{AMDVLK\_PROFILE\_INSTR\_GEN} and other variables to a profile file path
	\item Run the application $n$ times which produces an output file on each run
	\item Merge the $n$ files for each pipeline
	\item Set \texttt{AMDVLK\_PROFILE\_INSTR\_USE} to the path of the merged files (and more variables) and run the application again
\end{enumerate}
The paths to profile data files should contain a \texttt{\%i} which will get replaced by the pipeline id when data is loaded and stored.
When generating data, \texttt{\%m} can also be used to insert a unique id by LLVM into the filename.

The following options can be changed by environment variables:
\begin{itemize}
	\item \texttt{AMDVLK\_PROFILE\_INSTR\_GEN}: Generate instrumentation in shader code, use e.g. \texttt{Pipeline\_\%i\_\%m.profraw}
	\item \texttt{AMDVLK\_PROFILE\_INSTR\_USE}: Optimize shader code, use e.g. \texttt{Pipeline\_\%i.profdata}
	\item \texttt{AMDVLK\_PROFILE\_LATE}: Add PGO passes after structurization, default is before \texttt{StructurizeCFG}
	\item \texttt{AMDVLK\_PROFILE\_PER\_WAVE}: Count executions per SIMD unit instead of per lane, only use this in combination with late instrumentation!
	\item \texttt{AMDVLK\_PROFILE\_ANALYSIS}: Add analysis passes and write results to temporary file
	\item \texttt{AMDVLK\_PROFILE\_UNIFORM}: Add passes to generate and use instrumentation for uniform variables
	\item \texttt{AMDVLK\_PROFILE\_REMOVE}: Remove basic blocks which are never executed, this has only an effect when using PGO data
	\item \texttt{AMDVLK\_PROFILE\_NON\_ATOMIC}: Use normal counters instead of atomic counters for instrumentation
\end{itemize}

The detailed effect of these variables and how they are implemented is described in the following sections.

\subsection{Compiling Shaders}
\label{sub:compiling}
When the environment variable \texttt{AMDVLK\_PROFILE\_INSTR\_GEN} is set, we insert instrumentation into shaders. There, we have to differ between two possibilities:
In the default case, we replace any \texttt{\%i} in the filename with the pipeline id and provide this filename to the \texttt{PassManagerBuilder}.
Otherwise, if the variable \texttt{AMDVLK\_PROFILE\_LATE} is set, we want to insert the instrumentation after the \texttt{StructurizeCFG} pass.
Therefore, we add all needed PGO passes in the \texttt{AMDGPUTargetMachine} pass, after structurization.

To use the most basic instrumentation with the \texttt{PassManagerBuilder}, we only have to set one of its member variables to the requested filename from within LLPC.
To support more of our use cases, we added a member variable of the class \texttt{InstrProfOptions} to the \texttt{PassManagerBuilder}.
This allows us to set more options from LLPC instead of within the \texttt{PassManagerBuilder} only.
One of these options decides if atomic counters are used or not.
We set this property to true except if the environment variable \texttt{AMDVLK\_PROFILE\_NON\_ATOMIC} is set.
Another change in the \texttt{PassManagerBuilder} for our implementation is that we add our new PGO passes if necessary.
For example the analysis pass or the uniformity instrumentation generation pass.

When instrumenting after structurization, \texttt{AMDGPUTargetMachine} adds all needed passes.
It handles the same options like atomic counters but without the \texttt{PassManagerBuilder}, so the counter instrumentation and use passes have to be added explicitly.

The PGO passes in LLVM generate an ELF file with additional data and code sections that contain e.g. basic block counters and the registered destructor to write the collected data to disk.
We cannot directly use the destructor generated by LLVM because code on the GPU does not have access to the file system on the host, so we deactivate this code.
Our solution to save the collected data is described in \cref{sub:save}.

\subsection{Loading Shaders}
\label{sub:loading}
After LLPC and LLVM in the AMDVLK driver compiled the code, PAL is responsible for loading the shader ELF files.
This means copying the compiled code to GPU memory and running the shaders.
The previous ELF support of PAL was limited as it only extracted the \texttt{.text} and \texttt{.data} section (code and data) and uploaded them to GPU memory.
For PGO instrumentation, we also need to load additional data sections for counters and metadata.
As part of this work, we implemented loading all sections in an ELF file which are marked with write or execution flags.

To increment counters, the code in the compiled object file references the counters in the data section.
As the final addresses are not yet known at compile-time, LLVM adds relocation entries for them as described in \cref{sub:elf}.
We implemented applying these relocations in PAL. The steps in PAL to load a shader are:
\begin{enumerate}
	\item Allocate memory for all sections on the GPU so we know their final addresses
	\item Copy sections to the allocated GPU memory by mapping the GPU memory to the CPU
	\item Iterate through all relocation sections and perform the relocations
\end{enumerate}
When performing the relocations, we read from the loaded ELF file on the CPU and write to the GPU mapped memory.
Reading from GPU mapped memory would mean we have to wait until the data arrives through the PCIe bus, which takes longer than reading from CPU memory.
On the other hand, writing is fine because we do not have to wait until data is actually written, we can just continue, thus hiding the latency.

PAL's ELF parser contained a bug so it only supported \texttt{sh\_link} and \texttt{sh\_info} references to already loaded sections. This bug was fixed as part of this thesis.

%The string table for a given symbol table is available in the table's section header in \texttt{sh\_link}.

\subsection{Fetching and Storing Data}
\label{sub:save}
After loading and running a shader, we have to save the counter values.
As described in \cref{sub:save-design}, we cannot create the data files from inside the shader.
Instead, the driver has to fetch the data from GPU memory and write it to files.

Most parts of the file writing code of LLVM are inside a library.
The library gets statically linked to the compiled program when PGO instrumentation is enabled.
This does not happen for shaders because they are never linked in the current implementation.
We decided to link that library into PAL, so the driver can access the file writing code.
The needed information to write a file are the addresses and sizes of the PGO related ELF sections.
In PAL, we can read this information from the pipeline ELF file. We map the sections from the GPU memory to the CPU, write their addresses into global variables and call the LLVM PGO file dump function.
We explicitly specify the filename and replace any \texttt{\%i} inside the name with the current pipeline hash.

There is a single problem with counting on the GPU and dumping the data from the CPU:
The metadata contains the absolute memory addresses of the counters and LLVM also dumps the absolute address of the counter section.
The counter addresses are set through relocations when loading the ELF file which means they are virtual addresses on the \emph{GPU}.
The address of the counter section on the other hand is a virtual address on the \emph{CPU} which maps to GPU memory. To get a correct dump file, these addresses need to refer to the same address space.
Therefore, before writing the data to a file, we replace the GPU virtual addresses in the metadata with CPU virtual addresses.

\subsection{Loading and Using Profile Data}
\label{sub:load}
Similar to compiling shaders, the LLVM \texttt{PassManagerBuilder} needs the filename with PGO data.
As before, we replace a \texttt{\%i} in the filename with the pipeline hash before handing the name to LLVM.
LLVM will then add passes to set the basic block frequency information, which is used by subsequent passes for optimizations.

A problem that turned up when using profiling data is the \texttt{ControlHeightReduction} pass~\cite{ControlHeightReduction}.
This pass is specific to profile-guided optimizations and it duplicats some basic blocks and --- based on a condition --- executes one or the other.
For some shaders in Dota 2, it duplicated the last block which contains the \texttt{export} intrinsic call.
This intrinsic is responsible for exporting a position in the vertex shader or a final color in the pixel shader.
While the \texttt{export} instruction still gets executed once per SIMD lane after the block duplication, it now gets executed multiple times per SIMD unit.
This leads to problems, because it has a meaning for the whole SIMD unit, e.g. the \texttt{done} flag tells the GPU that this is the last \texttt{export} instruction on this unit.
Executing multiple \texttt{export} instructions with the \texttt{done} flag set leads to GPU hangs.
To fix this bug, we marked the \texttt{export} intrinsic as \emph{convergent} which means it shall not depend on additional control flow and we modified the \texttt{ControlHeightReduction} pass so it does not duplicate regions which contain convergent intrinsics.

\subsection{Basic Block Counters}
\label{sub:bbcounters}
Counting executions of basic blocks is implemented in three different LLVM passes.
The pass \texttt{PGOInstrumentationGen} inserts increment intrinsics into some basic blocks.
On the other side, \texttt{PGOInstrumentationUse} loads a \texttt{.profdata} file, reads the counters and sets the block frequency data.
Creating the counter data sections, storing the metadata and emitting the actual add instructions happens in the \texttt{InstrProfiling} pass.
This pass iterates over all counter intrinsics from the \texttt{PGOInstrumentationGen} pass and replaces them by add instructions.

These passes are already part of LLVM. We only modified the \texttt{InstrProfiling} pass to support per unit counters.
The details of this change are described in \cref{sub:impl_per_unit}.

\subsection{Unused Code}
\label{sub:impl_unused_code}
After collecting basic block counters, we are able to find code which is never executed.
To get this information, we could look into the captured profiling dumps and search for counters which are zero. But not every basic block has its own counter, as some counters can be computed from others.
As we do not want to miss these blocks, we wrote LLVM passes instead.
One pass which needs information about unused code is our analysis pass, which we use to collect information for diagrams in this thesis.
Another pass is responsible for removing unused basic blocks if the environment variable \texttt{AMDVLK\_PROFILE\_REMOVE} is set.
These passes run after the \texttt{PGOInstrumentationUse} pass so they have access to the block frequency information.

As described in the design section, because of inaccuracies, not every unused basic block contains zero in the counter when we access it.
Therefore, if a basic block is executed less than $< \SI{0.001}{\percent}$ of entry block executions, it is considered unused.
This does not work perfectly for unused blocks in loops, where the inaccuracy could be greater, but it seems to suffice in our case.

Before removing an unused block, we need to remove all references to this block.
If we find references in a conditional branch instruction, we can remove the reference to the unused block and transform the instruction to an unconditional branch.
There are more complicated cases, e.g. an unconditional branch to the block we want to remove or references in a switch.
In these cases, we replace the reference with a reference to a new basic block that only contains the unreachable intrinsic.
Then, when no more references exist, the unused block is removed.

\subsection{Uniform Branches}
\label{sub:impl_uniform_branches}
The design section for uniform branches explained that detecting the uniformity of branches is equal to testing if the branch condition is a uniform variable.
The implementation details to find out if a variable is uniform or not are explained in \cref{sub:impl_uniform_vars}.
This section explains peculiarities when applying this to conditions and branches.

As with basic block counters, we can capture the uniformity of branches before or after the \texttt{StructurizeCFG} pass or corresponding to that, per SIMD lane or per unit.
If we capture before structurizing, we should count the amount of lanes that are affected by a divergent variable.
So when a SIMD unit currently has 14 active lanes and the variable of interest is not uniform across these lanes, we add 14 to the counter.
On the other hand if we insert the counters after structurization, we want to count the units instead of lanes.
This means for a non-uniform variable, we add one to the counter, no matter how many lanes are active.

The uniformity of branches is a special case when using this instrumentation after structurization.
The structurization is the pass which handles and eliminates divergent branch instructions, thus afterwards, every branch is statically known to be uniform.
Hence, after the structurization pass ran, we cannot inspect the normal conditions of branches anymore because they do not represent the branches from the shader code.
Instead, we have to take a look at what happens with conditions in the \texttt{StructurizeCFG} pass.
The original conditions are used as an argument for the \texttt{amdgcn.if} intrinsic.
This intrinsic returns a boolean that signals if the whole unit can skip the branch which is then used for the branch instruction.
Therefore, if we are instrumenting branches before structurization, we directly instrument the condition arguments of branches.
If we are instrumenting after structurization, we look at the argument of \texttt{amdgcn.if} intrinsics instead.

To find out, if a branch is statically known to be uniform, we use the \texttt{DivergenceAnalysis} before structurization.
This analysis tells us for a condition, if the variable is uniform or divergent.
In single cases, it may not work perfectly, e.g. if a function argument for the shader is not known to be uniform at this stage in the compilation pipeline but overall it is working good.
After structurization, we know that if a block contains the \texttt{if} intrinsic, the condition is divergent.
On the other hand, if a block contains neither the \texttt{if} intrinsic, nor an \texttt{amdgcn.else} or \texttt{amdgcn.loop} intrinsic, it must be uniform.
At the moment, only ifs are instrumented after structurization, loops are ignored.

\subsection{Uniform Variables}
\label{sub:impl_uniform_vars}
To track the uniformity of variables, we use simple counters that stay zero for a uniform variable or get incremented for a divergent variable.
For the counters, we can reuse much of the code from basic block counters.
We can emit the same intrinsics as \texttt{PGOInstrumentationGen} and the counter sections and more complex instructions will be handled by the \texttt{InstrProfiling} pass.
We only have to take care that the counters for uniformity do not conflict with basic block counters.
Thus, we assign the uniform counters to a new (nonexistent) function, with \texttt{-uniform} appended to the original function name.

For example if we want to check if a variable \texttt{\%0} is uniform, we emit the following code:
\begin{lstlisting}[caption={Check a variable for uniformity},language={[x86masm]Assembler}]
%1 = tail call i32 @llvm.amdgcn.readfirstlane(i32 %0)
; The 33 is the comparison code for non-equal
; We compare %0 != %1 and get the result for all lanes
%2 = tail call i64 @llvm.amdgcn.icmp.i32(i32 %0, i32 %1, i32 33)
%3 = icmp ne i64 %2, 0
%4 = zext i1 %3 to i64
; This intrinsic also takes metadata, like the function name
; We add %4 to the counter (0 if uniform, 1 if divergent)
call void @llvm.instrprof.increment.step(i8* getelementptr inbounds ([23 x i8], [23 x i8]* @__profn__amdgpu_vs_main_uniform, i32 0, i32 0), i64 0, i32 18, i32 0, i64 %4)
\end{lstlisting}

At the moment, we instrument conditions to find uniform branches and \texttt{LoadInstr}s.
The interesting parts of a load instruction are the address --- we want to know if we can use a scalar load --- and the loaded memory value.
This gives a coarse insight into how applications use memory loads and how diverse the content of their memory is.

To create statistics about the uniformity of variables, we use the same analysis pass as for unused code.
The \texttt{DivergenceAnalysis} tells us if a variable is statically known to be uniform.
If a variable is not statically uniform, it is dynamically uniform if the counter is zero or divergent if the counter is unequal to zero.

\subsection{Analyses}
\label{sub:impl_analysis}
Statistics are collected by a new LLVM pass called \texttt{PGOInstrumentationAnalysis}.
This pass uses the information from annotations and writes the data to a log file.
This log file is then processed by a Python script to aggregate the data for diagrams and tables.

One part of the collected statistic is the amount of unused basic blocks as described in \cref{sub:impl_unused_code}.
These statistics lead to the evaluation in \cref{sub:eval_counters}.
Other information which are collected through this pass are data about uniform variables.
An instrumented variable can be either statically uniform, dynamically uniform or divergent.
The most interesting case are dynamically uniform variables: They are not detected by the compiler to be uniform, but at runtime they turn out to be uniform.

There is one more part of the analysis which is not part of the LLVM pass.
For debugging, the driver has the ability to dump all used pipelines to a folder.
The dumps are in a human readable form and contain the assembly code for the shaders and associated metadata.
We use these files to get the number of registers used by the shaders.

\subsection{Per Unit Counting}
\label{sub:impl_per_unit}
In the usual case, when simple atomic counters are used, we get one atomic operation per lane.
This means each active lane will start an atomic add to the same memory location at the same time.
These simultaneous atomics are quite slow and should be optimized to a single atomic operation per unit.
When instrumenting before structurization, this could be accomplished by the atomic optimizer pass.
Even if the atomic optimizer currently does not optimize the atomics for instrumentation, its purpose is to fuse atomic operations into a single operation per unit.
After structurization however, the atomic optimizer already ran, and we need another way.
Also, counting per unit needs an atomic operation which gets executed on a single lane.

We implemented per unit counting in the \texttt{InstrProfiling} pass, which is responsible for lowering the profiling increment intrinsics.

An easy way to restrict the execution to a single lane is to set the EXEC mask to one. This means only the first lane will be active.
The code in \cref{lst:set_exec} accomplishes this with only three instructions overhead.
\begin{lstlisting}[caption={Restricting execution to a single SIMD unit},label=lst:set_exec,language={[x86masm]Assembler}]
; %1 = EXEC
%1 = call i64 @llvm.read_register.i64(metadata !1) #1
; EXEC = 1
call void @llvm.write_register.i64(metadata !1, i64 1) #1
; Atomic increment
%2 = atomicrmw add i64* getelementptr inbounds ([3 x i64], [3 x i64]* @__profc__amdgpu_ps_main, i64 0, i64 1), i64 1 monotonic
; Restore the EXEC register from %1
call void @llvm.write_register.i64(metadata !1, i64 %1) #1

attributes #1 = { convergent }
!1 = !{!"exec"}
\end{lstlisting}

Unfortunately, these instructions do not mark any control flow changes for LLVM so they can get reordered and mess up the rest of the code.
After structurization, the code mostly stays in order, however in some cases it still gets reordered, leading to graphical artifacts.
To get stable code for incrementing counters on one lane only, we reuse code from the atomic optimizer.
This code uses a few intrinsics to create a condition which will only be true for the first line.
Then it creates an if construct based on the condition where the if-block will only be run for a single lane.
It expects however to be run before the structurization pass, which will transform the introduced branches.
To be able to use this code in per unit counting, we adjust the emited code in that case.
If late instrumentation is active, the \texttt{InstrProfiling} pass directly creates code using the condition modifying intrinsics from structurization.
I.e. it emulates the \texttt{StructurizeCFG} pass to run afterwards.

The resulting code is more robust to LLVM reordering but also less efficient.
Also it does not count blocks in the per unit mode when the EXEC mask is zero, so if no SIMD lane is active but the scalar unit may still compute something.
It results in five instructions overhead when run before structurization and a few more when run afterwards, also depending on the shader type (pixel shaders need special treatment when helper lanes are in use).
