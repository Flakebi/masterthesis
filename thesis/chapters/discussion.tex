\clearpage
\bigsection{Discussion}
\label{sec:discussion}

\subsection{Instrumentation before and after Structurizing the CFG}
\label{sub:discussion_structurize}
The \texttt{StructurizeCFG} pass in the LLVM compiler pipeline is a turning point. Before, we look at everything from the perspective of a single thread (or lane).
After this pass, the control-flow graph contains the per SIMD unit view, where an if-else was before, both branches can now be executed after each other.
It is important to note that the per unit perspective only applies to the CFG after this pass. All instructions apart from branch instructions stay the same and exhibit the per lane view.

For profile-guided optimizations, we have to decide if we want to instrument the CFG before or after structurization. In the case of basic block counters, this will lead to different results.
Before structurization, it is impossible to run through both branches of an if-else construct. After structurization, it is still impossible \emph{per lane}, but it is possible in the \emph{per unit} perspective.
If we count executions per lane, instrumenting before or after structurization does not make a semantic difference.
After \texttt{StructurizeCFG}, more counters are needed, because there are more basic blocks and LLVM does not know anymore that only one block of an if-else construct will be entered.
On the other hand if we count executions per unit, it does make a difference. More importantly, counting executions per unit before the CFG gets structurized leads to wrong results as LLVM expects than an else-block will not be entered if the if-block is executed. And this assumption is false for the per unit point of view.

Summarizing, there are two different possibilities for instrumentation. First, counting per lane, where we should instrument before structurization because it leads to fewer counters and makes no difference otherwise.
And second, counting per SIMD unit after structurization.
These two opportunities represent the per lane and per unit view on a GPU.
The decision for the instrumentation is the same as for actually using collected data. If we collect counters before the \texttt{StructurizeCFG} pass, we need to annotate the results at the same point in the pipeline.
LLVM will propagate them when transforming the CFG in the structurization.
If we collect counters after structurization, we also need to annotate them afterwards. This means, the collected data is only available to the compiler after the \texttt{StructurizeCFG} pass.
Looking at CPUs, LLVM applies instrumentation and usage of counters at the end of the pipeline, after most optimizations.
This makes sense because the counters are used for linearization and getting data about an unoptimized CFG is not useful if the final CFG that needs to be linearized looks completely different.

The advantage of a per lane instrumentation is simplicity. The \texttt{PassManagerBuilder} can insert the needed passes easily (it runs before structurization) and we do not need to adjust any instrumentation code because the LLVM IR exhibits the per lane view.

To use per unit instrumentation, we need to add passes manually after structurization and cannot use the \texttt{PassManagerBuilder}.
Additionally, we need to change the PGO instrumentation pass that the increment instruction is only run on one lane and not on all lanes.
An advantage of the per unit view is that it is more similar to the code that gets executed on the hardware in the end.
If we compare two basic blocks, one always gets run by the whole unit, for the other only a single lane is active, e.g. to aggregate a result over the whole unit.
When we use per lane counting, the first basic block will be weighted a lot more important than the second one.
Per unit counting provides a more accurate view, both blocks are executed the same amount of times, so they are equally important. The amount of active lanes does not matter.

\subsection{Related Work}
\label{sub:relatedwork}
A part of this work is to profile shaders. There are many GPU profiling tools~\cite{UnityGPUProfiler, UnrealGPUProfiling, MSGPUUsage, PGI2014}.
Most of them show the needed computation time down to single draw calls or overall GPU performance statistics like occupancy\footnote{The amount of simultaneously executed threads on a \gls{cu}, hiding memory latency},used memory bandwidth and compute power~\cite{NvidiaNsight, NvidiaShaderPerf, AMDShaderAnalyzer}.
There are only few, which show detailed information about the inside of shaders. E.g GPUOcelot can trace single instructions in shaders when emulating CUDA applications on the GPU~\cite{GPUOcelot, Lakshminarayana2010}. Pyramid is an emulator and shader analyzer for various architectures, including AMD GCN~\cite{Pyramid}.
While emulation can provide accurate information, we cannot run our application at the same speed as on the GPU as the CPU has much less computational power.
Arm has something~\cite{Barton2013}.
In the high-performance computing space, they used Lynx~\cite{Lynx} to instrument CUDA applications and automatically measure performance statistics~\cite{Farooqui2014}.

Benchmarks: Cuda SDK, Parboil, Rodinia, SHOC, Tensor but not Vulkan so we cannot use it. VComputeBench claims to be a general, publicly available benchmark for GPGPU~\cite{Mammeri2018}. However, we were unable to find these benchmarks online and the author did not respond to our questions.

\subsection{Future Work}
\label{sub:futurework}
This work does provide a good step into PGO on GPUs but of course it is not complete.
This section will explain possible directions for future work in this field.

When analyzing uniformity, we rely on the divergence analysis to give us information about uniform or divergent variables.
As mentioned before, this analysis does not yield perfectly accurate results at all steps in the compilation pipeline.
An opportunity for future work is to look into these problems, fix them where possible and find the best point to run this analysis.

The uniformity analysis does only analyze conditions and values from memory at the moment.
It would be interesting to expand this to image and buffer loads.
They make up a considerable part of the memory accesses in a shader, but analyzing them is more difficult and did not fit into the time frame of this thesis.

In addition to analyzing if a variable is uniform or not, we can also create statistics about how often a variable is uniform.
If the divergent cases only make up a low fraction of the executions, it can still be beneficial to optimize for the common case.
At the moment, we do not know how often this is the case.

Based on the basic block counters, we removed basic blocks which never get executed.
This worked good for two games but did not work for others.
As removing basic blocks was meant as a simple and dirty test, this is not too surprising.
However, it would be nice to have this test working for more games and collect more information about the effects.

A related bug in LLVM are the artifacts which show up in Ashes when using PGO.
We found one bug caused by PGO on GPUs in the \texttt{ControlHeightReduction} pass, we did not find the cause for this bug though.
Before PGO can be used in production environments, this bug should be fixed and more games should be tested with these optimizations.

Now, that we have methods to collect runtime data from shaders, this data can be used in the compiler to improve the optimization of shaders.
This includes many of the optimizations which we suggested in \cref{sub:optimizations}.
Hoisting loads out of conditional blocks for example is a promising candidate for rewarding optimizations.

The existing optimizations were applied to five different games.
The collected performance data give an impression of what we can expect of PGO on graphics cards.
There exist many more games with different engines and they all use the GPU in a slightly different way.
Thus, it would be interesting to compare the performance of PGO of more games and other Vulkan applications.

\subsection{Conclusion}
\label{sub:conclusion}
To our knowledge, this is the first work which leverages profile-guided optimizations on GPUs.

%Actual results.
Basic block counting (implemented in LLVM, needed adjustments)
-> unused code statistics
Analyze register usage in different configurations
Unused code: Registers and performance

Register usage of Dota: Unchanged after removing nearly 20\% of the code
Remove unused code: Switch-vm + \SI{21.06 \pm 0.10}{\percent}

Condition/Branch uniformity
Most often, the biggest category are static uniform branches
varies a lot between games

Uniformity of loads
The fractions of the three categories for static uniform, dynamic uniform and divergent values vary a lot between games.
Divergent loads are the smallest category, under \SI{35}{\percent}.
For Ashes and Warhammer, dynamic uniform loads are the common case. For the rest of the games, static uniform loads make up more than half of the loads.

In the end, we discussed if PGO instrumentation should be used before or after the CFG structurization.
Result: After structurization is better
