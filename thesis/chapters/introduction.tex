%\clearpage
\bigsection{Introduction}
This section motivates the optimization of code running on the \gls{gpu} and explain why runtime analysis is needed for some optimizations. We follow that the knowledge of runtime information is beneficial for the performance of \gls{gpu} programs. In the end we list our contributions.

\subsection{Motivation}
\label{sub:motivation}
\paragraph{History of GPUs} \Glspl{gpu} started as simple co-processors, able to draw polygons on a frame buffer, which is displayed on a screen, but relying on the \gls{cpu} for everything else. As time proceeded, more parallelism was introduced and functionality was added to move more computations from the CPU to the GPU. GPUs became capable of drawing more complex images for games with light and shadows, though the functionality was baked into the hardware. The CPU was responsible for putting data into the \emph{fixed function pipeline} which then did its configurable but not programmable job.
As more and more functionality was added to graphics processors, game developers -- among others -- got the possibility to write small programs called shaders, which are run on the GPU. Simultaneously, the hardware changed from a very limited set of hardware backed functions to a more general processor model. Nowadays, the computing power of GPUs stems from their large amount of general purpose arithmetical processing units which can be freely programmed. Much of the functionality which was previously baked into the hardware is now implemented in software running on these general processors.~\cite{McClanahan2010}

Soon after the possibility to program GPUs with shaders came up, people started to use them for other purposes than graphics. While this was not the primary purpose of shaders, there is now good support in APIs and libraries to use GPUs for many purposes. Today, the highly parallel computing power is used for games, high performance computing and even things like offloading computations from spreadsheets or databases~\cite{Lillqvist2016, Meraji2015}. This usage of graphics processors is called \gls{gpgpu}.

\paragraph{Optimization} In general, we try to execute code as fast as possible. This saves time, energy and money or allows us to handle larger problems. To achieve this goal, most of the code that we execute first runs through multiple stages of compilation and optimization. The time when optimizations happen differs depending on the used programming languages and tools.
E.g. C++ code is optimized ahead of runtime, when compiling code. On the other hand, scripting languages like JavaScript or bytecode languages like Java and C\texttt{\#} are optimized at runtime by a \gls{jit}.
On a GPU, code is compiled ahead of runtime, similar to C++. The compiled code is then copied over the \gls{pciebus} to the GPU and executed.

For ahead-of-time compiled programming languages, optimizations are generally based on static analyses of code. For example constant propagation can be implemented by first analyzing the possible values of variables using a fixed point iteration over the \gls{cfg} and afterwards transforming the \gls{cfg} such that known variable values are used.~\cite{Seidl2010}

Another possible optimization is the decision where to place code in a resulting executable. It is beneficial for performance if seldom executed instructions are put after the return statement. On an average execution, these instructions are not executed. If we would put them inside other code, we would have to jump over them at some point. If we instead put them after the return statement, we can spare this one jump in the average case and are thus faster. The general process of deciding the order of code in the executable is called \emph{linearization}~\cite{Seidl2010}. The compiler has to decide a linear order of the code, which is previously represented as a control flow graph.

There is a difficulty with this optimization though: The compiler often does not know which branch will be taken (hot) and which code is reached seldom (cold). Therefore it has several means to figure out which branches are hot and which cold. Most often, the compiler uses heuristics. For example error handling code is probably taken less often than a path without errors. As another variant, the code author themselves can tell the compiler which branch is taken more often. An example is the Linux kernel, which often makes use of \texttt{likely} and \texttt{unlikely} markers in \texttt{if}-conditions.

There are cases though, where the outcome of a condition depends on the concrete input into the application or the compilers heuristics take the wrong guess. In theory, a developer could work around this by marking every of those conditions with \texttt{likely}. In general this is undesirable as we want the compiler to automate as much as possible and developers can be mistaken when guessing the likelihood of branches.
For \gls{jit} compilers, which are working at runtime, this optimization is not a problem because the interpreter knows which branches are hot and which cold. The knowledge of concrete input data enables them to output more efficient code than ahead-of-time compilers.

\paragraph{Profile-Guided Optimization} There is a remedy for compiled languages though: We can provide sample input data to the compiler. Then an ahead-of-time compiler has access to the same information as a \gls{jit} compiler and it can make use of information e.g. about hot branches to generate more efficient code. Popular C and C++ compilers like clang and msvc implement this in the form of \gls{pgo}~\cite{ClangManual, MicrosoftPgo}. This optimization can yield performance improvements of more than \SI{10}{\percent} in some cases~\cite{LarabelPgo2018}.

\Cref{lst:compile-pgo} shows an exemplary usage of \gls{pgo}. The procedure consists of three different steps:
\begin{enumerate}
	\item At first, the program is compiled with static optimizations only and instrumentation instructions are inserted.
	\item As a second step, the compiled program is run on sample input data. In this step, the instrumentation code collects useful information about the execution and usually creates a file containing this data. In the case of clang, this data has to be post-processed with the \texttt{llvm-profdata} executable.
	\item In the third and last step, the program is compiled again. This time, the compiler has access to collected runtime/profiling information so it can produce a better optimized version of our application.
\end{enumerate}

\lstinputlisting[language=bash, caption={Compiling a C++ program with \glsdesc{pgo} using clang.}, label={lst:compile-pgo}, float]{figures/compile-pgo.sh}

For clang there is also a second option available. Instead of using instrumentation to capture runtime information, we can also use standard profiling tools like the Linux \texttt{perf} tool~\cite{LinuxPerf}. Profilers like \texttt{perf} generally incur a lower runtime overhead compared to instrumentation. The downside is that the resulting information is less detailed~\cite{ClangManual}.

The usage example in \cref{lst:compile-pgo} shows that using \gls{pgo} today is well-supported by popular tools and easy to use for developers.
In the first part of this introduction, we looked at the evolution of GPUs and how they became highly parallel, general purpose. Doing profile-guided optimizations however, is not yet supported for GPU code. The problem is the lack of tools for profiling or instrumentation of shaders.

\paragraph{GPU Profiling} There exist many analysis tools to analyze shader performance~\cite{RenderDoc, NvidiaShaderPerf}. They give information about how long a shader needs to execute or how much of the memory bandwidth it uses. But almost all of them stop at this level and are not able to give instruction-level insights into the performance of shader code like where it spends the most time, which branches are taken, how long different memory fetches take, etc.
There exist static analysis tools that approximate this information~\cite{AMDShaderAnalyzer}. However, as seen before with the comparison of ahead-of-time and \glspl{jit} compilers, the knowledge of concrete input data enables some optimizations that are otherwise impossible.

Before we can start to implement more sophisticated optimizations in the compiler that rely on profiling data, we first need detailed profiling information. The focus of this thesis is to collect the necessary runtime information of GPU code, to use it for PGO. In the workflow of PGO, this is part of step one, namely inserting instrumentation when compiling code for the first time% in forever
. The collected information from the second step can then be fed back into the compiler for use in optimizations.

\subsection{Graphics Processing Unit}
To understand how profiling data can -- and cannot -- be collected on GPUs, we need a basic understanding of how code is run on graphics processors. This section gives a short explanation of the underlying hardware and in which points it differs from CPUs.
Afterwards we take a closer look at the communication between CPU and GPU. This is important to collect runtime information about shaders because this is the part where we need to fetch and store collected analysis data. The hardware section is largely based on a blog series about compute shaders by Matth√§us Chajdas~\cite{Chajdas2018}.

The general concepts, approaches and results of this thesis should apply to all GPU vendors. For simplicity and because of the publicly available resources and code, we will focus the work on AMD graphics cards and their open source Vulkan driver AMDVLK.

To understand why GPUs are built the way they are, it is helpful to understand how developers write GPU code. Similar to CPU code, developers like to write simple code that does computations on a single data point. Much of the hard work like choosing instructions or parallelizing loops with \gls{simd} instructions can be done automatically by the compiler. This works similarly for GPU code. A developer writes code for e.g. computing the color of one pixel in a game. The compiler then chooses the right instructions and in the end the GPU does the computation for each pixel of a rendered triangle. The way \gls{simd} instructions are used on CPUs and GPUs is quite different though.

On a CPU, \gls{simd} instructions are often used to vectorize multiple iterations of a loop that are independent of each other. An equivalent way of expressing this would be to write SIMD instructions by hand. In summary, multiple instructions of the code a developer writes (which may be from multiple loop iterations) are fused into one SIMD instruction.

On a GPU however, loops are not vectorized. Instead, a program gets duplicated for multiple input data. For games this means the program is started multiple times -- one time per pixel -- to compute the color for every pixel. If the program flow is the same for all instances and only the input values are different, the GPU runs (almost) every instruction of a shader on a SIMD unit. The multiple input and output data of the SIMD unit are from multiple instances (pixels) of the shader. As we will see later, this poses a problem if the program flow is not the same. On CPUs this concept of parallelism is similar to different threads, which only have limited access to each other and run the same code on different data.

\paragraph{Hardware} This section describes the hardware architecture of AMD \gls{gcn} GPUs as this is the architecture we are most familiar with. The general concept of SIMD units is the same on NVIDIA graphic cards, though a difference is e.g. that there are 32 concurrent floating-point operations while there are 64 on AMD \gls{gcn} cards.

On the hardware level, the basic building block of a modern GPU is a big \gls{simd} unit. The same instruction is run on $16 \times$ 32-bit floating-point numbers simultaneously. We will call these 16 different numbers \emph{lanes}. For most instructions, the pipeline has 4 stages and an instruction takes 4 cycles to complete. This also means that after the first cycle, the GPU can issue a second instruction, computing on another 16 floating-point numbers. This could be the next instruction in the program but then the GPU would have to care about data dependencies between instructions. This would add a lot of complexity so AMD decided to do something else instead. They virtually enlarge the SIMD unit to work on a multiple of 16 numbers and execute the same instruction again. The GPU can execute the same instruction 4 times, then the first instruction will be finished in the next cycle. This means at one point in the pipeline, there are 4 instructions in flight, each computing on 16 numbers, which makes a total of 64 different lanes. For a developer, this looks like a SIMD unit which runs computations on 64 floats in parallel.

In addition to the SIMD unit, a \gls{gcn} GPU contains a few other processing units. One of them is the \emph{scalar unit}. This unit comes along the SIMD units, so a program that is run on the GPU contains both, vector instructions for the SIMD units and scalar instructions for the scalar unit in an interleaved manner. The scalar unit is meant for computations which are constant among all SIMD lanes. Using the scalar unit instead of SIMD units is a lot more efficient in memory and energy because we only need to store data once instead of 64 times, and we only need to execute operations only once instead of 64 times. Data values which are the same for a whole SIMD unit are called \emph{uniform}, in contrast to \emph{non-uniform} values which are different in each lane. The scalar unit is also responsible for managing the control flow of the SIMD unit. The vectorized instructions do not support jumps because a jump cannot be executed on a subset of lanes. Instead, jump instructions are executed on the scalar unit. We will come back to that later, with a more detailed explanation of branching.

Another unit on a \gls{gcn} GPU is the instruction scheduler. It is responsible for fetching new instructions and handing them to the responsible unit, e.g. the scalar unit or the SIMD unit. For our current setup with one SIMD unit, one scalar unit and one instruction scheduler, the scalar unit will only get an instruction every 4 cycles because it works in synchronization with the SIMD unit and the instruction scheduler will only output instructions every 4\textsuperscript{th} cycle. The only unit which is occupied the whole time is the vector unit. For this reason, every scalar unit and instruction scheduler is responsible for 4 SIMD units. A pack of 4 SIMD units, one scalar unit and one instruction scheduler is called \gls{cu}. Additionally, one \gls{cu} contains some local memory called \gls{lds}. One GPU consists of multiple such packs, for example a recent AMD Radeon VII GPU contains 60 \glspl{cu}. For a floating-point instruction such as an addition, which takes 4 cycles, this accumulates to a total of $60 \cdot 4 = 240$ SIMD units and a maximum of $240 \cdot 64 = 15360$ SIMD computations running in parallel. Every cycle, $240 \cdot 16 = 3840$ will get ready. And as a computation takes 4 cycles, 15360 are currently \enquote{in-flight}.

As emphasized before, having divergent control flow is not simply possible for program instances which get executed on one SIMD unit. For this purpose, AMD \gls{gcn} GPUs have an \emph{EXEC mask}, which contains one bit for every lane of a SIMD unit. If this bit is one, it means a lane is active, if it is zero, a lane is inactive and all SIMD operations are a no-op for this lane. When the program encounters a branch instruction, there are several possibilities. If all lanes take one branch, the scalar unit can execute a jump and execution continues as normal. If a part of the lanes take one branch and the rest takes another branch, both branches have to be executed. Typically, the execution mask for the first branch is set and the first branch is executed. Then the execution mask is flipped and the second branch is executed. After both branches are executed, the execution mask is reset to all ones again.

\paragraph{Software} This section explains how developers run code on GPUs. Developers which write code for GPUs do not have to handle all details of the hardware. Similar to CPUs where the operating system initializes the processors when starting and handles the resource allocations of processes, GPUs are managed by a \emph{driver}. The driver abstracts from the concrete underlying hardware and gives developers an interface that is easier to use.

Similar to different operating systems and standards like POSIX and the Windows \gls{api}, there exist different interfaces to communicate with graphic drivers. Two open standards, which are implemented by multiple vendors, are OpenGL and Vulkan. A specialty in comparison to CPUs is that the compiler is integrated into the driver in many APIs. For example in OpenGL this would be accomplished with the \texttt{glShaderSource} and \texttt{glCompileShader} function. In Vulkan there is \texttt{vkCreateShaderModule} for this purpose.

In this section we explain the structure of compiling and running shaders with the AMDVLK Vulkan driver on Linux. This driver is based on the LLVM compiler framework. Typically, shaders for Vulkan are written in the \gls{glsl}. The developer first has to translate this source coded to the intermediate language \emph{SPIR-V}. The resulting SPIR-V code can be passed to the Vulkan API. From there is passed to the LLPC (the llvm pipeline compiler). The SPIR-V code gets then converted into the LLVM \gls{ir}. After some intermediate transformations on this \gls{ir}, the LLVM compiler gets called. In this part most optimizations happens.
The LLVM framework also converts the IR to another IR, the \emph{SelectionDAG} -- as the name says a directed, acyclic graph of instructions~\cite{llvmSelectionDag}. The SelectionDAG gets linearized and transformed into \emph{MachineIR}~\cite{llvmSelectionDag}. MachineIR is the intermediate representation which is most similar to the native \gls{isa} of the graphics card. The conversion to the \gls{isa} happens in the last step. The output of LLVM is a file in the \gls{elf}. The LLPC applies some patches to the generated \gls{elf} file and then returns it.

When running a shader, the compiled \gls{elf} file is passed to the \gls{pal}. This library is responsible for talking to the operating system and the in-kernel part of the driver also called \gls{kmd}. It reads meta-information from the ELF file and uploads the compiled code to the GPU to execute it. The stages to run code on a GPU thus look like this:

\begin{enumerate}
	\item Developer writes human readable \gls{glsl} code
	\item \texttt{glslangValidator} converts \gls{glsl} to SPIR-V bytecode
	\item Developer passes bytecode to Vulkan driver with \texttt{vkCreateShaderModule}
	\item LLPC converts SPIR-V to LLVM IR code
	\item LLPC transforms the LLVM IR to make it valid for the LLVM framework
	\item LLVM optimizes the IR and converts it to the SelectionDAG
	\item LLVM transforms the SelectionDAG to MachineIR
	\item LLVM converts MachineIR to GPU ISA and returns an ELF file
\end{enumerate}
The compilation is done now, the next steps are loading and executing the code.
\begin{enumerate}
	\setcounter{enumi}{8}
	\item PAL reads the ELF file and loads it onto the GPU
	\item The GPU runs the code
\end{enumerate}

\subsection{Optimizations}
The goal of this thesis is to build the instrumentation, which is used in the first step of the PGO workflow. The instrumentation collects information at runtime and is needed to perform optimizations afterwards. Before we can start collecting data, we need to know which information we need. Therefore, it is part of this thesis is to find out which information can enable optimizations and should be collected and also how they can be collected.

The needed data is defined by the optimizations we want to perform in step three. In the following, we list some possible optimizations and the respectively needed profiling data. They are collected from various sources, e.g. PGO optimizations for CPUs~\cite{MicrosoftPgo} and tailored to GPUs, as there are some peculiarities which have to be taken into account. Some described optimizations are specific to the current architecture of GPUs and have no close counterpart on CPUs.

\subsubsection{Improve linearization}
\paragraph{Needed data} Branch probabilities
\paragraph{Description} As described in the introduction, moving seldom taken branches after the return instruction improves the performance.
\paragraph{Background} This optimization is implemented in LLVM~\cite{llvmLinearization}. To avoid generating worse code, it is only applied when profiling data is available or the compiler assigned a rather high probability to the hot branch.
\paragraph{Example}\ \\%TODO Use gpu instructions
\begin{minipage}{.47\textwidth}
\begin{lstlisting}[caption={Linearization -- unoptimized},frame=tlrb,language={[x86masm]Assembler}]
cmp a, b
jle else
; then-branch
jmp endif
else:
; else-branch
endif:
ret
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.47\textwidth}
\begin{lstlisting}[caption={Linearization -- optimized},frame=tlrb,language={[x86masm]Assembler}]
cmp a, b
jle else
; then-branch
endif:
ret    ; no jump if the condition is true
else:
; else-branch
jmp endif
\end{lstlisting}
\end{minipage}

\subsubsection{Skip branches}
\paragraph{Needed data} Branch probabilities and distribution on a SIMD unit
\paragraph{Description} Only insert the branch skip instruction for the scalar unit if often all or no thread take the branch. Otherwise, do not insert it as both branches are executed anyway.
\paragraph{Background} This optimization is an example which has no counterpart for CPUs. CPUs do not have the ability to run SIMD instructions only on a part of the lanes.
\paragraph{Example}
	
\subsubsection{Prefetch Textures from Branches}
\paragraph{Needed data} Branch probabilities
\paragraph{Description} Texture fetches that happen in branches are sometimes moved before the branch by the compiler -- so executed unconditionally. This has the effect that the shader can do some work and then use the loaded texture value when it is available. The optimization speeds up the execution if the branch condition evaluates to true. However, if the condition evaluates to false, it will degrade performance because the GPU unnecessarily fetches memory.
\paragraph{Background} Prefetching textures is already done but without any information about how often a texture is needed.
\paragraph{Example}

\subsubsection{Value Profiling}
\paragraph{Needed data} Variable value distribution
\paragraph{Description} If a variable often has the same value, we can introduce a branch and apply constant propagation to provide a fast path. This increases the code size as parts of the code are duplicated but can speed up the shader.
\paragraph{Background} The concept of value profiling exists since 1997~\cite{Calder1997}. But as there is no detailed profiling for GPUs so far, this was only done for CPUs.
\paragraph{Example} Blend multiple textures based on a material texture which mostly contains 0. So we make a branch for != 0 and do texture fetches/computations only there. (Combine newly created branches with branch probabilities?)
% Throw away transparent blocks on textures?

\subsubsection{Detect Uniform Computations}
\paragraph{Needed data} Variable value distribution on a SIMD unit
\paragraph{Description} If we do a computation or memory fetch on the same value for every lane on a SIMD unit, i.e. it is uniform, the code can be optimized to be executed only once by the scalar unit instead of the vector unit.
\paragraph{Background} This is similar to value profiling but specialized to GPUs. We do not look at isolated executions of a program. Instead, we inspect if values are differing across one parallel execution.
\paragraph{Example} Address values are the same on a wave, so we fetch memory with the scalar unit.

\subsection{Notation}
\label{sub:notation}
There exist quite a few different notations for the different logical and physical parts of a GPU. Different vendors invent different terms and CPU notations like \enquote{thread} can be ambiguous when used in the context of GPUs. The logically similar meaning of a thread on a GPU would be a single lane for the SIMD units. On the other hand, a thread could be a description for a complete SIMD unit which executes one program at a time, which makes sense from a hardware point of view.



Best notation for SIMD-lanes, PAL, \glspl{cu}.

\subsection{Contributions}
\label{sub:contributions}
