%\clearpage
\bigsection{Introduction}
In this section we motivate the optimization of code running on the \gls{gpu} and explain why runtime analysis is needed for some optimizations. We follow that the knowledge of runtime information is beneficial for the performance of \gls{gpu} programs. In the end we list our contributions.

\subsection{Motivation}
\label{sub:motivation}
\paragraph{History of GPUs} \Glspl{gpu} started as simple co-processors, able to draw polygons on a frame buffer, which is displayed on a screen, but relying on the \gls{cpu} for everything else. As time proceeded, more parallelism was introduced and functionality was added to move more computations from the CPU to the GPU. GPUs became capable of drawing more complex images for games with light and shadows, though the functionality was baked into the hardware and the CPU was responsible for putting data into the \emph{fixed function pipeline} which then did its configurable but not programmable job.
As more and more functionality was added to graphics processors, game developers -- among others -- got the possibility to write small programs called shaders, which are run on the GPU. Simultaneously, the hardware changed from a very limited set of hardware backed functions to a more general processor model. Nowadays, the computing power of GPUs stems from their large amount of general purpose arithmetical processing units which can be freely programmed. Much of the functionality which was previously baked into the hardware is now implemented in software running on these general processors.~\cite{McClanahan2010}

Soon after the possibility to program GPUs with shaders came up, people started to use them for other purposes than graphics. While this was not the primary purpose of shaders, there is now good support in APIs and libraries to use GPUs for many purposes. This usage of graphics processors is called \gls{gpgpu}. Today, the highly parallel computing power is used for games, high performance computing and even things like offloading computations from spreadsheets or databases.~\cite{Lillqvist2016, Meraji2015}

\paragraph{Optimization} In general, we try to execute code as fast as possible. This saves time, energy and money or allows us to handle larger problems. To achieve this goal, most of the code that we execute first runs through multiple stages of compilation and optimization. Depending on the used programming languages and tools, the time when optimizations happen differ, e.g. C++ code is optimized ahead of runtime, when compiling code, while scripting languages like JavaScript or bytecode languages like Java and C\texttt{\#} are optimized at runtime by a \gls{jit}. Code that is executed on a GPU is compiled ahead of runtime, similar to C++. The compiled code is then copied to the GPU over the \gls{pciebus} and executed.

For ahead-of-time compiled programming languages, optimizations are generally based on static analyses of the code. For example constant propagation can be implemented by first analyzing the possible values of variables using a fixed point iteration over the \gls{cfg} and afterwards transforming the \gls{cfg} such that known variable values are used.~\cite{Seidl2010}

Another example where a compiler can optimize code is the decision where to place code in the resulting executable. It is beneficial for performance if seldom executed instructions are put after the return statement. On an average execution, these instructions are not executed. If we would put them inside other code, we would have to jump over them at some point. If we instead put them after the return statement, we can spare this one jump in the average case and are thus faster. The general process of deciding the order of code in the executable is called \emph{linearization}~\cite{Seidl2010}. The compiler has to decide a linear order of the code, which is previously represented as a control flow graph.

There is a difficulty with this optimization though. The compiler often does not know which branch will be taken (hot) and which code is reached seldom (cold). The compiler has several means to figure this out. Most often, the compiler uses heuristics to figure out which branch is taken more likely. For example error handling code is probably taken less often than a path without errors. As another variant, the code author themself can tell the compiler which branch is taken more often. An example is the Linux kernel, which often makes use of \texttt{likely} and \texttt{unlikely} markers in \texttt{if}-conditions.

There are cases though, where the outcome of a condition depends on the concrete input into the application or the compilers heuristics take the wrong guess. A developer could work around this by marking conditions with \texttt{likely} but this is in general undesirable as we want to automate as much as possible and developers can be mistaken when guessing the likelihood of branches. For \gls{jit} compilers, which are working at runtime, this is no problem because they know which branches are hot and which cold. The knowledge of concrete input data enables them to output more efficient code than ahead-of-time compilers.

\paragraph{Profile-Guided Optimization} There is a remedy for compiled languages though: We can provide sample input data to the compiler. An ahead-of-time compiler then has access to the same information as a \gls{jit} compiler, so it can make use of information e.g. about hot branches to generate more efficient code. Popular C and C++ compilers like clang and msvc implement this in the form of \gls{pgo}~\cite{ClangManual, MicrosoftPgo}.

An exemplary usage of \gls{pgo} can be seen in \cref{lst:compile-pgo}. The procedure consists of three different steps:
\begin{enumerate}
	\item At first, the program is compiled with static optimizations only and instrumentation instructions are inserted.
	\item As a second step, the compiled program is run on some sample input data. In this step, the instrumentation code collects useful information about the execution and usually creates a file containing this data. In the case of clang, this data has to be post-processed with the \texttt{llvm-profdata} executable.
	\item In the third and last step, the program is compiled again. This time, we provide the collected runtime/profiling information so the compiler can produce a better optimized version of our application.
\end{enumerate}

\lstinputlisting[language=bash, caption={Compiling a C++ program with \glsdesc{pgo} using clang.}, label={lst:compile-pgo}, float]{figures/compile-pgo.sh}

For clang there is also a second possibility available. Instead of using instrumentation to capture runtime information, we can also use some standard profiling tools like the Linux \texttt{perf} tool~\cite{LinuxPerf}. Profilers like \texttt{perf} generally incur a lower runtime overhead compared to instrumentation. The downside is that the resulting information is less detailed~\cite{ClangManual}.

In the usage example in \cref{lst:compile-pgo}, it can be seen that today, using \gls{pgo} is well-supported by popular tools and easy to use for developers. In the first part of this introduction, we looked at the evolution of GPUs and how they became highly parallel, general purpose processors. Doing profile-guided optimizations however, is not yet supported for GPU code. The problem is the lack of tools for profiling or instrumentation of shaders.

\paragraph{GPU Profiling} There exist many analysis tools to analyze shader performance~\cite{RenderDoc, NvidiaShaderPerf}. For example, they give information about how long a shader needs to execute or how much of the memory bandwidth it uses. But almost all of them stop at this level and are not able to give instruction-level insights into the performance of shader code like where does it spend the most time, which branches are taken, how long different memory fetches take, etc. There exist static analysis tools that approximate this information~\cite{AMDShaderAnalyzer}. However, as seen before with the comparison of ahead-of-time and \glspl{jit} compilers, the knowledge of concrete input data enables some optimizations that are otherwise not possible.

Before we can start to implement more sophisticated optimizations in the compiler that rely on profiling data, we first need detailed profiling information. The focus of this thesis is to collect the necessary runtime information of GPU code, to use it for PGO. In the workflow of PGO, this is part of step one, namely inserting instrumentation when compiling code for the first time% in forever
. The collected information from the second step can then be fed back into the compiler for use in optimizations.

\subsection{Graphics Processing Unit}
To understand how profiling data can -- and cannot -- be collected on GPUs, we need a basic understanding of how code is run on graphics processors. This section gives a short explanation of the underlying hardware and in which points it differs from CPUs. Afterwards we take a closer look at the communication between CPU and GPU. This is important to collect runtime information about shaders because this is the part, where we need to fetch and store collected analysis data. The hardware section is largely based on a blog series about compute shaders by Matth√§us Chajdas~\cite{Chajdas2018}.

The general concepts, approaches and results of this thesis should apply to all GPU vendors. For simplicity and because of the publicly available resources and code, we will focus the work on AMD graphics cards.

To understand why GPUs are built the way they are, it is helpful to understand how developers write GPU code. Similar to CPU code, developers like to write simple code that does computations on a single data point. Much of the hard work like choosing instructions or parallelizing loops with \gls{simd} instructions can be done automatically by the compiler. This works similarly for GPU code. A developer writes code for e.g. computing the color of one pixel in a game. The compiler then chooses the right instructions and in the end the GPU does the computation for each pixel of a rendered triangle. The way \gls{simd} instructions are used on CPUs and GPUs is quite different though.

On a CPU, \gls{simd} instructions are often used to vectorize multiple iterations of a loop that are independent of each other. An equivalent way of expressing this would be to write SIMD instructions by hand. In summary, multiple instructions of the code a developer writes (which may be from multiple loop iterations) are fused into one SIMD instruction.

On a GPU however, loops are not vectorized. Instead, our program gets duplicated for multiple input data. For games this means the program is started multiple times -- one time per pixel -- to compute the color for every pixel. If the program flow is the same for all instances and only the input values are different, the GPU runs (almost) every instruction of a shader on a SIMD unit. The multiple input and output data of the SIMD unit are from multiple instances (pixels) of the shader. As we will see later, this poses a problem if the program flow is not the same. On CPUs this concept of parallelism is similar to different threads, which only have limited access to each other and run the same code on different data.

\paragraph{Hardware} This section is describes the hardware architecture of AMD \gls{gcn} GPUs as this is the architecture we are most familiar with. The general concept of SIMD units is the same on NVIDIA graphic cards, though a difference is e.g. that we result in 32 concurrent floating-point operations while there are 64 on AMD \gls{gcn} cards. On the hardware level, the basic building block of a modern GPU is a big \gls{simd} unit. The same instruction is run on $16 \times$ 32-bit floating-point numbers simultaneously. We will call these 16 different numbers \emph{lanes}. For most instructions, the pipeline has 4 stages and an instruction takes 4 cycles to complete. This also means that after the first cycle, we can issue a second instruction, computing on another 16 floating-point numbers. We do not want to care about data dependencies between instructions because it adds a lot of complexity. So we virtually enlarge our SIMD unit to work on a multiple of 16 numbers and execute the same instruction again. We can do execute the same instruction 4 times, then the first instruction will be finished in the next cycle. This means at one point in our pipeline, there are 4 instructions in flight, each computing on 16 numbers, which makes a total of 64 different lanes. For a developer, this looks like a SIMD unit which runs computations on 64 floats in parallel.

In addition to the SIMD unit, a \gls{gcn} GPU contains a few other processing units. One of them is the \emph{scalar unit}. This processor comes along the SIMD units, so a program that is run on the GPU contains both, vector instructions for the SIMD units and scalar instructions for the scalar unit in an interleaved manner. The scalar unit is meant for computations which are constant among all SIMD lanes. Using the scalar unit instead of SIMD units is a lot more efficient in memory and energy because we only need to store data once instead of 64 times, and we only need to execute operations only once instead of 64 times. Data values which are the same for a whole SIMD unit are called \emph{uniform}, in contrast to \emph{non-uniform} values which are different in each lane. The scalar unit is also responsible for managing the control flow of the SIMD unit. The vectorized instructions do not support jumps because a jump cannot be executed on a subset of lanes. Instead, jump instructions are executed on the scalar unit. We will come back to that later, with a more detailed explanation of branching.

Another unit on a \gls{gcn} GPU is the instruction scheduler. It is responsible for fetching new instructions and giving them to the responsible unit, e.g. the scalar unit or the SIMD unit. For our current setup with one SIMD unit, one scalar unit and one instruction scheduler, the scalar unit will only get an instruction every 4 cycles because it works in synchronization with the SIMD unit and the instruction scheduler will only output instructions every 4\textsuperscript{th} cycle. The only unit which is occupied the whole time is the vector unit. For this reason, every scalar unit and instruction scheduler is responsible for 4 SIMD units. A pack of 4 SIMD units, one scalar unit and one instruction scheduler is called \gls{cu}. Additionally, one \gls{cu} contains some local memory called \gls{lds}. One GPU consists of multiple such packs, for example a recent AMD Radeon VII GPU contains 60 \glspl{cu}. For a floating-point instruction such as an addition, which takes 4 cycles, this accumulates to a total of $60 \cdot 4 = 240$ SIMD units and a maximum of $240 \cdot 64 = 15360$ SIMD computations running in parallel. Every cycle, $240 \cdot 16 = 3840$ will get ready. And as a computation takes 4 cycles, 15360 are currently \enquote{in-flight}.

As emphasized before, having divergent control flow is not simply possible for program instances which get executed on one SIMD unit. For this purpose, AMD \gls{gcn} GPUs have an \emph{EXEC mask}, which contains one bit for every lane of a SIMD unit. If this bit is one, it means a lane is active, if it is zero, a lane is inactive and all SIMD operations are a no-op for this lane. When the program encounters a fork, there are several possibilities. If all lanes take one branch, the scalar unit can execute a jump and execution continues as normal. If a part of the lanes take one branch and the rest takes another branch, both branches have to be executed. Typically, the execution mask for the first branch is set and the first branch is executed. Then the execution mask is flipped and the second branch is executed. After both branches are executed, the execution mask is reset to all ones again.

\paragraph{Software} This section explains how code is run on GPUs. Developers, which write code for GPUs, do not communicate directly with the hardware but through software which abstracts from the concrete underlying hardware. For CPUs this would be the operating system, which initializes the processors when starting and handles the resource allocations of processes. For GPUs, this part is done by the driver.

Similar to different operating systems and standards like POSIX and the windows \gls{api}, there exist different interfaces to communicate with graphic drivers. Two open standards, which are implemented by multiple vendors, are OpenGL and Vulkan. A specialty in comparison to CPUs is that the compiler is integrated into the driver in many APIs. For example in OpenGL this would be accomplished with the \texttt{glShaderSource} and \texttt{glCompileShader} function. In Vulkan there is \texttt{vkCreateShaderModule} for this purpose.

In this section we explain the structure of compiling and running shaders with the AMDVLK Vulkan driver on Linux. This driver is based on the LLVM compiler framework. When a shader compilation is invoked through the Vulkan API, the shader code is passed to the LLPC (the pipeline compiler). All code input, e.g. in \gls{glsl} format, is first converted to SPIR-V bytecode which is the standardized intermediate representation for Vulkan shaders. The SPIR-V code gets then converted into the LLVM \gls{ir}. After some intermediate transformations on this \gls{ir}, the LLVM compiler gets called. This is the part where all the optimization happens. The LLVM framework also has the task to convert the IR to another IR, the \emph{SelectionDAG} -- as the name says a directed, acyclic graph of instructions -- and finally to the native \gls{isa} of the graphics card. The output of LLVM is a file in the \gls{elf}. The LLPC applies some patches to the generated \gls{elf} file and then returns it.

When running a shader, the compiled \gls{elf} file is passed to the \gls{pal}. This library is responsible for talking to the operating system and the in-kernel part of the driver. It reads meta-information from the ELF file and uploads the compiled code to the GPU to execute it.

\subsection{Optimizations}
The goal of this thesis is to build the instrumentation, which is used in the first step of the PGO workflow. The instrumentation collects information at runtime, which is needed to perform optimizations afterwards. Before we can start collecting data, we need to know which information we need. Therefore, it is part of the thesis is to find out which information can enable optimizations and should be collected and also how they can be collected.

The needed data is defined by the optimizations we want to perform in step three. In the following, we list some possible optimizations and the respectively needed profiling data. They are collected from various sources, e.g. PGO optimizations for CPUs~\cite{MicrosoftPgo} and tailored to GPUs, as there are some peculiarities which have to be taken into account. Some described optimizations are specific to the current architecture of GPUs and have no close counterpart on CPUs.

\subsubsection{Improve linearization}
\paragraph{Needed data} Branch probabilities
\paragraph{Description} As described in the introduction, moving seldom taken branches after the return instruction improves the performance.
\paragraph{Background} This optimization is implemented in LLVM~\cite{llvmLinearization}. To avoid generating worse code, it is only applied when profiling data is available or the compiler assigned a rather high probability to the hot branch.
\paragraph{Example}\ \\%TODO Use gpu instructions
\begin{minipage}{.47\textwidth}
\begin{lstlisting}[caption={Linearization -- unoptimized},frame=tlrb,language={[x86masm]Assembler}]
cmp a, b
jle else
; then-branch
jmp endif
else:
; else-branch
endif:
ret
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.47\textwidth}
\begin{lstlisting}[caption={Linearization -- optimized},frame=tlrb,language={[x86masm]Assembler}]
cmp a, b
jle else
; then-branch
endif:
ret    ; no jump if the condition is true
else:
; else-branch
jmp endif
\end{lstlisting}
\end{minipage}

\subsubsection{Skip branches}
\paragraph{Needed data} Branch probabilities and distribution on a SIMD unit
\paragraph{Description} Only insert the branch skip instruction for the scalar processor if often all or no thread take the branch. Otherwise, do not insert it as both branches are executed anyway.
\paragraph{Background} This optimization is an example which has no counterpart for CPUs. CPUs do not have the ability to run SIMD instructions only on a part of the lanes.
\paragraph{Example}
	
\subsubsection{Prefetch Textures from Branches}
\paragraph{Needed data} Branch probabilities
\paragraph{Description} Texture fetches that happen in branches are sometimes moved before the branch by the compiler -- so executed unconditionally. This has the effect that the shader can do some work and then use the loaded texture value when it is available. The optimization speeds up the execution if the branch condition evaluates to true. However, if the condition evaluates to false, it will degrade performance because the GPU unnecessarily fetches memory.
\paragraph{Background} Prefetching textures is already done but without any information about how often a texture is needed.
\paragraph{Example}

\subsubsection{Value Profiling}
\paragraph{Needed data} Variable value distribution
\paragraph{Description} If a variable often has the same value, we can introduce a branch and apply constant propagation to provide a fast path. This increases the code size as parts of the code are duplicated but can speed up the shader.
\paragraph{Background} The concept of value profiling exists since 1997~\cite{Calder1997}. But as there is no detailed profiling for GPUs so far, this was only done for CPUs.
\paragraph{Example} Blend multiple textures based on a material texture which mostly contains 0. So we make a branch for != 0 and do texture fetches/computations only there. (Combine newly created branches with branch probabilities?)
% Throw away transparent blocks on textures?

\subsubsection{Detect Uniform Computations}
\paragraph{Needed data} Variable value distribution on a SIMD unit
\paragraph{Description} If we do a computation or memory fetch on the same value for every lane on a SIMD unit, i.e. it is uniform, the code can be optimized to be executed only once by the scalar processor instead of the vector processors.
\paragraph{Background} This is similar to value profiling but specialized to GPUs. We do not look at isolated executions of a program. Instead, we inspect if values are differing across one parallel execution.
\paragraph{Example} Address values are the same on a wave, so we fetch memory with the scalar unit.

\subsection{Notation}
\label{sub:notation}
There exist quite a few different notations for the different logical and physical parts of a GPU. This is because of different vendors inventing different terms and ambiguous usage of CPU notations like \enquote{thread}. The logical similar meaning of a thread on a GPU would be a single lane for the SIMD units. On the other hand, a thread could be a description for a complete SIMD unit, which makes sense from a hardware point of view.



Best notation for (SIMD-)threads, \glspl{cu}.

\subsection{Contributions}
\label{sub:contributions}
