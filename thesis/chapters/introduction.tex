%\clearpage
\bigsection{Introduction}
This section motivates the optimization of code running on the \gls{gpu} and explain why runtime analysis is needed for some optimizations. We follow that the knowledge of runtime information is beneficial for the performance of \gls{gpu} programs. In the end we list our contributions.

\subsection{Motivation}
\label{sub:motivation}
\paragraph{History of GPUs} \Glspl{gpu} started as simple co-processors, able to draw polygons on a frame buffer, which is displayed on a screen, but relying on the \gls{cpu} for everything else. As time proceeded, more parallelism was introduced and functionality was added to move more computations from the CPU to the GPU. GPUs became capable of drawing more complex images for games with light and shadows, though the functionality was baked into the hardware. The CPU was responsible for putting data into the \emph{fixed function pipeline} which then did its configurable but not programmable job.
As more and more functionality was added to graphics processors, game developers -- among others -- got the possibility to write small programs called shaders, which are run on the GPU. Simultaneously, the hardware changed from a very limited set of hardware backed functions to a more general processor model. Nowadays, the computing power of GPUs stems from their large amount of general purpose arithmetical processing units which can be freely programmed. Much of the functionality which was previously baked into the hardware is now implemented in software running on these general processors.~\cite{McClanahan2010}

Soon after the possibility to program GPUs with shaders came up, people started to use them for other purposes than graphics. While this was not the primary purpose of shaders, there is now good support in APIs and libraries to use GPUs for many purposes. Today, the highly parallel computing power is used for games, high performance computing and even things like offloading computations from spreadsheets or databases~\cite{Lillqvist2016, Meraji2015}. This usage of graphics processors is called \gls{gpgpu}.

\paragraph{Optimization} In general, we try to execute code as fast as possible. This saves time, energy and money or allows us to handle larger problems. To achieve this goal, most of the code that we execute first runs through multiple stages of compilation and optimization. The time when optimizations happen differs depending on the used programming languages and tools.
E.g. C++ code is optimized ahead of runtime, when compiling code. On the other hand, scripting languages like JavaScript or bytecode languages like Java and C\texttt{\#} are optimized at runtime by a \gls{jit}.
On a GPU, code is compiled ahead of runtime, similar to C++. The compiled code is then copied over the \gls{pciebus} to the GPU and executed.

For ahead-of-time compiled programming languages, optimizations are generally based on static analyses of code. For example constant propagation can be implemented by first analyzing the possible values of variables using a fixed point iteration over the \gls{cfg} and afterwards transforming the \gls{cfg} such that known variable values are used.~\cite{Seidl2010}

Another possible optimization is the decision where to place code in a resulting executable. It is beneficial for performance if seldom executed instructions are put after the return statement. On an average execution, these instructions are not executed. If we would put them inside other code, we would have to jump over them at some point. If we instead put them after the return statement, we can spare this one jump in the average case and are thus faster. The general process of deciding the order of code in the executable is called \emph{linearization}~\cite{Seidl2010}. The compiler has to decide a linear order of the code, which is previously represented as a control flow graph.

There is a difficulty with this optimization though: The compiler often does not know which branch will be taken (hot) and which code is reached seldom (cold). Therefore it has several means to figure out which branches are hot and which cold. Most often, the compiler uses heuristics. For example error handling code is probably taken less often than a path without errors. As another variant, the code author themselves can tell the compiler which branch is taken more often. An example is the Linux kernel, which often makes use of \texttt{likely} and \texttt{unlikely} markers in \texttt{if}-conditions.

There are cases though, where the outcome of a condition depends on the concrete input into the application or the compilers heuristics take the wrong guess. In theory, a developer could work around this by marking every of those conditions with \texttt{likely}. In general this is undesirable as we want the compiler to automate as much as possible and developers can be mistaken when guessing the likelihood of branches.
For \gls{jit} compilers, which are working at runtime, this optimization is not a problem because the interpreter knows which branches are hot and which cold. The knowledge of concrete input data enables them to output more efficient code than ahead-of-time compilers.

\paragraph{Profile-Guided Optimization} There is a remedy for compiled languages though: We can provide sample input data to the compiler. Then an ahead-of-time compiler has access to the same information as a \gls{jit} compiler and it can make use of information e.g. about hot branches to generate more efficient code. Popular C and C++ compilers like clang and msvc implement this in the form of \gls{pgo}~\cite{ClangManual, MicrosoftPgo}. This optimization can yield performance improvements of more than \SI{10}{\percent} in some cases~\cite{LarabelPgo2018}.

\Cref{lst:compile-pgo} shows an exemplary usage of \gls{pgo}. The procedure consists of three different steps:
\begin{enumerate}
	\item At first, the program is compiled with static optimizations only and instrumentation instructions are inserted.
	\item As a second step, the compiled program is run on sample input data. In this step, the instrumentation code collects useful information about the execution and usually creates a file containing this data. In the case of clang, this data has to be post-processed with the \texttt{llvm-profdata} executable.
	\item In the third and last step, the program is compiled again. This time, the compiler has access to collected runtime/profiling information so it can produce a better optimized version of our application.
\end{enumerate}

\lstinputlisting[language=bash, caption={Compiling a C++ program with \glsdesc{pgo} using clang.}, label={lst:compile-pgo}, float]{figures/compile-pgo.sh}

For clang there is also a second option available. Instead of using instrumentation to capture runtime information, we can also use standard profiling tools like the Linux \texttt{perf} tool~\cite{LinuxPerf}. Profilers like \texttt{perf} generally incur a lower runtime overhead compared to instrumentation. The downside is that the resulting information is less detailed~\cite{ClangManual}.

The usage example in \cref{lst:compile-pgo} shows that using \gls{pgo} today is well-supported by popular tools and easy to use for developers.
In the first part of this introduction, we looked at the evolution of GPUs and how they became highly parallel, general purpose. Doing profile-guided optimizations however, is not yet supported for GPU code. The problem is the lack of tools for profiling or instrumentation of shaders.

\paragraph{GPU Profiling} There exist many analysis tools to analyze shader performance~\cite{RenderDoc, NvidiaShaderPerf}. They give information about how long a shader needs to execute or how much of the memory bandwidth it uses. But almost all of them stop at this level and are not able to give instruction-level insights into the performance of shader code like where it spends the most time, which branches are taken, how long different memory fetches take, etc.
There exist static analysis tools that approximate this information~\cite{AMDShaderAnalyzer}. However, as seen before with the comparison of ahead-of-time and \glspl{jit} compilers, the knowledge of concrete input data enables some optimizations that are otherwise impossible.

Before we can start to implement more sophisticated optimizations in the compiler that rely on profiling data, we first need detailed profiling information. The focus of this thesis is to collect the necessary runtime information of GPU code, to use it for PGO. In the workflow of PGO, this is part of step one, namely inserting instrumentation when compiling code for the first time% in forever
. The collected information from the second step can then be fed back into the compiler for use in optimizations.

\subsection{Optimizations}
The goal of this thesis is to build the instrumentation, which is used in the first step of the PGO workflow. The instrumentation collects information at runtime and is needed to perform optimizations afterwards. Before we can start collecting data, we need to know which information we need. Therefore, it is part of this thesis is to find out which information can enable optimizations and should be collected and also how they can be collected.

The needed data is defined by the optimizations we want to perform in step three. In the following, we list some possible optimizations and the respectively needed profiling data. They are collected from various sources, e.g. PGO optimizations for CPUs~\cite{MicrosoftPgo} and tailored to GPUs, as there are some peculiarities which have to be taken into account. Some described optimizations are specific to the current architecture of GPUs and have no close counterpart on CPUs.

\subsubsection{Improve linearization}
\paragraph{Needed data} Branch probabilities
\paragraph{Description} As described in the introduction, moving seldom taken branches after the return instruction improves the performance.
\paragraph{Background} This optimization is implemented in LLVM~\cite{llvmLinearization}. To avoid generating worse code, it is only applied when profiling data is available or the compiler assigned a rather high probability to the hot branch.
\paragraph{Example}\ \\%TODO Use gpu instructions
\begin{minipage}{.47\textwidth}
\begin{lstlisting}[caption={Linearization -- unoptimized},frame=tlrb,language={[x86masm]Assembler}]
cmp a, b
jle else
; then-branch
jmp endif
else:
; else-branch
endif:
ret
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.47\textwidth}
\begin{lstlisting}[caption={Linearization -- optimized},frame=tlrb,language={[x86masm]Assembler}]
cmp a, b
jle else
; then-branch
endif:
ret    ; no jump if the condition is true
else:
; else-branch
jmp endif
\end{lstlisting}
\end{minipage}

% TODO
\subsubsection{Skip branches}
\paragraph{Needed data} Branch probabilities and distribution on a SIMD unit
\paragraph{Description} Only insert the branch skip instruction for the scalar unit if often all or no SIMD lanes take the branch. Otherwise, do not insert it as both branches are executed anyway.
\paragraph{Background} This optimization is an example which has no counterpart for CPUs. CPUs do not have the ability to run SIMD instructions only on a part of the lanes.
\paragraph{Example}
	
\subsubsection{Prefetch Textures from Branches}
\paragraph{Needed data} Branch probabilities
\paragraph{Description} Texture fetches that happen in branches are sometimes moved before the branch by the compiler -- so executed unconditionally. This has the effect that the shader can do some work and then use the loaded texture value when it is available. The optimization speeds up the execution if the branch condition evaluates to true. However, if the condition evaluates to false, it will degrade performance because the GPU unnecessarily fetches memory.
\paragraph{Background} Prefetching textures is already done but without any information about how often a texture is needed.
\paragraph{Example}

\subsubsection{Value Profiling}
\paragraph{Needed data} Variable value distribution
\paragraph{Description} If a variable often has the same value, we can introduce a branch and apply constant propagation to provide a fast path. This increases the code size as parts of the code are duplicated but can speed up the shader.
\paragraph{Background} The concept of value profiling exists since 1997~\cite{Calder1997}. But as there is no detailed profiling for GPUs so far, this was only done for CPUs.
\paragraph{Example} Blend multiple textures based on a material texture which mostly contains 0. So we make a branch for != 0 and do texture fetches/computations only there. (Combine newly created branches with branch probabilities?)
% Throw away transparent blocks on textures?

\subsubsection{Detect Uniform Computations}
\paragraph{Needed data} Variable value distribution on a SIMD unit
\paragraph{Description} If we do a computation or memory fetch on the same value for every lane on a SIMD unit, i.e. it is uniform, the code can be optimized to be executed only once by the scalar unit instead of the vector unit.
\paragraph{Background} This is similar to value profiling but specialized to GPUs. We do not look at isolated executions of a program. Instead, we inspect if values are differing across one parallel execution. Using scalar instructions yields great benefits as described in this paper by Chen~\cite{Chen2016}.
\paragraph{Example} Address values are the same on a SIMD unit, so we fetch memory with the scalar unit.

\subsection{Notation}
\label{sub:notation}
There exist quite a few different notations for the different logical and physical parts of a GPU. Different vendors invent different terms and CPU notations like \enquote{thread} can be ambiguous when used in the context of GPUs. The logically similar meaning of a thread on a GPU would be a single lane for the SIMD units. On the other hand, a thread could be a description for a complete SIMD unit which executes one program at a time, which makes sense from a hardware point of view.

Throughout this paper, we will use the term SIMD lane for a single work-item. As a wavefront (AMD) or warp (NVIDIA) is run on a SIMD unit, we will use the expression SIMD unit.

%We will use the term \gls{cu} used by OpenCL and AMD, called \gls{sm} by NVIDIA.
% cite https://community.khronos.org/t/relation-between-cuda-cores-and-compute-units/4772/2

\subsection{Contributions}
\label{sub:contributions}
