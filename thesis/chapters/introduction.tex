%\clearpage
\bigsection{Introduction}
In this section we motivate the optimization of code running on the \gls{gpu} and explain why runtime analysis is needed for some optimizations and thus beneficial for the performance of \gls{gpu} programs. Afterwards we list our contributions.

\subsection{Motivation}
\label{sub:motivation}
\Glspl{gpu} started as simple additional processors, able to draw polygons, but relying on the \gls{cpu} for everything else. As the time proceeded, more parallelism was introduced and functionality was added to move more computations from the CPU to the GPU. GPUs became capable of drawing more complex images for games with light and shadows, though the functionality was still baked into the hardware and the CPU was responsible to put data into the \emph{fixed function pipeline} which then did its configurable but not programmable job.
As more and more functionality was added to graphics processors, game developers got the possibility to write small programs (called shaders), which are run on the GPU. Simultaneously, the hardware changed from a very limited set of hardware backed functions to a more general processor model. Nowadays, the computing power of GPUs stems from their large amount of general purpose arithmetical processing units which can be freely programmed. Much of the functionality, which was previously baked into the hardware is now implemented in software running on more general processors.~\cite{McClanahan2010}

Soon after the possibility to program GPUs with shaders came up, people started to use GPUs for other purposes than graphics. While this was not the primary purpose of shaders, there is now good support in APIs and libraries to use GPUs for many purposes. This usage of graphics processors is called \gls{gpgpu}. Today, the highly parallel computing power is used for games, high performance computing and even things like offloading computations in spreadsheets or databases.~\cite{Lillqvist2016, Meraji2015}

In general, we try to execute code as fast as possible. This saves time, energy and money or allows us to handle larger problems. To achieve this goal, most of the code that we execute first runs through multiple stages of compilation and optimization. Depending on the used programming languages and tools, the time when optimizations happen differ, e.g. C++ code is optimized ahead of the runtime, when compiling the code, while scripting languages like JavaScript or bytecode languages like Java and C# are optimized at runtime by the \gls{jit}. Code that is executed on a GPU is compiled ahead of the runtime, similar to C++. At runtime, the compiled code is sent to the GPU over the \gls{pcie} bus and executed.

For ahead-of-time compiled programming languages, optimizations are generally based on static analyses of the code. For example constant propagation can be implemented by first analyzing the possible values of variables using a fixed point iteration on the \gls{cfg} and afterwards transforming the \gls{cfg} such that known variable values are used.~\cite{Seidl2010}

Another example where a compiler can optimize code is the decision, where to place code in the resulting executable. It is beneficial for the performance if seldom executed instructions are put after the return statement. On an average execution, these instructions are not executed. If we would put them inside the other code, we would have to jump over them at some point. If we instead put them after the return statement, we can spare this one jump in the average case and are thus faster. The general process of deciding the order of code in the executable is called \emph{linearization} because the compiler has to decide a linear order of the code, which is previously represented as a control flow graph.

There is a problem with this optimization though. The compiler often does not know which branch will be taken and which code is reached seldom. The compiler has several means to figure this out. Most often, the compiler uses heuristics to figure out which branch is taken more likely. For example error handling code is probably taken less often than the path without errors. As another variant, the code author himself can tell the compiler which branch is taken more often. An example is the Linux kernel, which often makes use of \texttt{likely} and \texttt{unlikely} markers in \texttt{if}-conditions.

There are cases though, where the outcome of a condition depends on the concrete input into the application or the compilers heuristics take the wrong guess. A developer could work around this by marking conditions with \texttt{likely} but this is in general undesirable as we want to automate as much as possible and developers can be mistaken when guessing the likelihood of branches. For \glspl{jit}, which are working at runtime, this is no problem because they know which branches are hot and which cold. The knowledge of concrete input data enables them to output more efficient code than ahead-of-time compilers.

There is a remedy for compiled languages though: What if we could provide sample input data to the compiler? The compiler then has access to the same information as the \gls{jit}, so he can make use of information e.g. about hot branches to generate more efficient code. Popular C and C++ compilers like clang and msvc implement this in the form of \gls{pgo}~\cite{ClangManual, MicrosoftPgo}.

\lstinputlisting[language=bash, caption={Compiling a C++ program with \glsdesc{pgo} using clang.}, label={lst:compile-pgo}, float]{figures/compile-pgo.sh}

An exemplary usage of \gls{pgo} can be seen in \cref{lst:compile-pgo}. The procedure consists of three different steps:
\begin{enumerate}
	\item At first, the program is compiled with static optimizations only and instrumentation instructions are inserted.
	\item As a second step, the compiled program is run on some sample input data. In this step, the instrumentation code collects useful information about the execution and usually creates a file containing this data. In the case of clang, this data has to be post-processed with the \texttt{llvm-profdata} executable.
	\item In the third and last step, the program is compiled again. This time, we provide the collected runtime/profiling information so the compiler can produce a better optimized version of our application.
\end{enumerate}

For clang there is also a second possibility available. Instead of using instrumentation to capture runtime information, we can also use some standard profiling tools like the Linux \texttt{perf} tool~\cite{LinuxPerf}. Profilers like \texttt{perf} generally incur a lower runtime overhead compared to instrumentation. The downside is that the resulting information is less detailed~\cite{ClangManual}.

In the usage example in \cref{lst:compile-pgo}, it can be seen that using \gls{pgo} today is well-supported by popular tools and easy to use for developers. In the first part of this introduction, we looked at the evolution of GPUs and how they became highly parallel, general purpose processors. Doing profile-guided optimizations however, is not yet supported for GPU code. The problem is the profiling or instrumentation of shaders.

There exist many analysis tools to analyze shader performance~\cite{RenderDoc, NvidiaShaderPerf}. They give information about how long a shader needs to execute, how much of the memory bandwidth it uses, etc. But almost all of them stop at this level and are not able to give insights into the shader code like where does it spend the most time, which branches are taken, how long do the different memory fetches take, etc. There exist static analysis tools that approximate the performance~\cite{AMDShaderAnalyzer}. However, as seen before with the comparison of ahead-of-time and \glspl{jit}, the knowledge of concrete input data enables some optimizations that are otherwise not possible and static analysis which is only based on shader code does not have access to concrete input data.

Before we can start to implement more sophisticated optimizations in the compiler that rely on profiling data, we first need detailed profiling information. The focus of this thesis is to collect the necessary runtime information of GPU code, to use it in PGO. In the workflow of PGO, this is part of step one, namely inserting the instrumentation when compiling the code for the first time% in forever
. This collected information can then be fed back into the compiler for use in optimizations.

\subsection{Graphics Processing Unit}
To understand how profiling data can (and cannot) be collected on GPUs, we need a basic understanding of how code is run on graphics processors. First, we will give a short explanation of the underlying hardware and in which points it differs from CPUs. Afterwards we take a closer look at the communication between CPU and GPU. This is important to collect runtime information about shaders because this is the part, where we need to fetch and store collected analysis data. The hardware part is largely based on a blog series about compute shaders by Matth√§us Chajdas~\cite{Chajdas2018}.

To understand why GPUs are built the way they are, it is helpful to understand how developers write GPU code. Similar to CPU code, developers like to write simple code that does computations on a single data point. Much of the hard work like choosing instructions or parallelizing loops with \gls{simd} instructions can be done automatically by the compiler. This works similarly for GPU code. A developer writes code for e.g. computing the color of one pixel in a game. The compiler then chooses the right instructions and in the end the GPU does the computation for each pixel of a rendered triangle. The way \gls{simd} instructions are used on CPUs and GPUs is quite different though.

On a CPU, \gls{simd} instructions are often used to vectorize multiple iterations of a loop that are independent of each other. An equivalent way of expressing this would be to write SIMD instructions by hand. In summary, multiple instructions of the code a developer writes (which may be from multiple loop iterations) are fused into one SIMD instructions.

On a GPU however, our loops are not vectorized. Instead, our program gets duplicated for multiple input data. For games this means the program is started multiple times -- one time per pixel -- to compute the color for every pixel. As the program flow is the same for all instances and only the input values are different, the GPU just runs (almost) every instruction of a shader on a SIMD unit. The multiple input and output data of the SIMD unit are from multiple instances (pixels) of the shader. On CPUs this concept of parallelism is similar to different threads, which only have limited access to each other and run the same code on different data.

On the hardware level, the basic building block of a modern GPU is a big \gls{simd} unit. On AMD cards, the same instruction is run on $16 \times$ 32-bit floating-point numbers simultaneously. We will call these 16 different numbers \emph{data streams}. For most instructions, the pipeline has 4 stages and an instruction takes 4 cycles to complete. This also means that after the first cycle, we can issue a second instruction, computing on another 16 floating-point numbers. We do not want to care about data dependencies between instructions, so we virtually enlarge our SIMD unit to work on a multiple of 16 numbers (concrete: 64) and execute the same instruction again. We can do this 4 times, then the first instruction will be finished. This means at one point in our pipeline, there are 4 instructions in flight, each computing on 16 numbers, which makes a total of 64 different data streams. For a developer, it will look like a SIMD unit which runs computations on 64 floats in parallel. In NVIDIA graphics cards, the same instruction is executed on 32 numbers.

In addition to the SIMD unit, a GPU contains a few other processing units. One of them is the \emph{scalar unit}. This processor comes along the SIMD units, so a program that is run on the GPU contains both, vector instructions for the SIMD units and scalar instructions for the scalar unit in an interleaved manner. The scalar unit is meant for computations which are constant among all SIMD data streams. Using the scalar unit instead of SIMD units is a lot more efficient in memory and energy because we only need to store data once instead of 64 times. Data values which are the same for a whole SIMD unit are called \emph{uniform}, in contrast to \emph{non-uniform} values which are different in each data stream. The scalar unit is also responsible for managing the control flow of the SIMD unit. The vectorized instructions do not support jumps because a jump cannot be executed on a subset of the data streams. Instead, jump instructions are executed on the scalar unit.

Another unit on a GPU is the instruction scheduler. It is responsible for fetching new instructions and giving them to the responsible unit, e.g. the scalar unit or the SIMD unit. For our current setup with one SIMD unit, one scalar unit and one instruction scheduler, the scalar unit will only get an instruction every 4 cycles because it works in synchronization with the SIMD unit and the instruction scheduler will only output instructions every 4\textsuperscript{th} cycle. The only unit which is occupied the whole time is the vector unit. For this reason, every scalar unit and instruction scheduler is responsible for 4 SIMD units.

Explain why branches are important.

The general concepts, approaches and results of this thesis should apply to all GPU vendors. For simplicity and because of the publicly available resources and code, we will focus the work on AMD graphics cards.

\subsection{Optimizations}
The goal of this thesis is to collect the necessary runtime information of GPU code (step two in the workflow of PGO). This is needed to perform optimizations afterwards. Before we can start collecting data, we need to know which information we need. The needed data is defined by the optimizations we want to perform in step three.
In the following, we list some possible optimizations and the respectively needed profiling data. They are collected from various sources, e.g. PGO optimizations for CPUs~\cite{MicrosoftPgo} and tailored to GPUs, as there are some specialties which have to be taken into account. Some described optimizations are specific to the current architecture of GPUs and have no close counterpart on CPUs.

\subsubsection{Improve linearization}
\paragraph{Needed data} Branch probabilities
\paragraph{Description} Pick the more probable branch first, etc.
\paragraph{Background} Already done for CPUs? Also, without PGO with likely/unlikely in gcc, \dots in msvc, something in gpus?
\paragraph{Example} With code: Two columns, left unoptimized, right optimized

\subsubsection{Skip branches}
\paragraph{Needed data} Branch probabilities and distribution per wave
\paragraph{Description} Only insert the branch skip instruction (scalar processor) if often all or no thread take the branch. Otherwise do not insert it as both branches are executed anyway.
\paragraph{Background}
\paragraph{Example}
	
\subsubsection{Decide prefetching of textures if they are loaded in branches}
\paragraph{Needed data} Branch probabilities
\paragraph{Description} Texture fetches that happen in branches are sometimes moved before the branch by the compiler -- so executed unconditionally. So the shader can do some work and then use the loaded texture value when it is available. This speeds up the execution if the branch condition evaluates to true. We only do the prefetching if the texture is needed with a probability $> 0.5$, otherwise we will not prefetch it.
\paragraph{Example}
	
Or if the condition is actually available before, we can do a short branch before to prefetch the texture.
\paragraph{Background} Prefetching textures is already done, but without any supporting data that it makes sense to prefetch the texture.
\paragraph{Example}
	
\subsubsection{Introduce conditions if a specific variable value occurs regularly}
\paragraph{Needed data} Variable value distribution
\paragraph{Description} If a variable often has the same value\dots Introduce branch and work with constant propagation. This increases the code size as parts of the code are duplicated but can speed up the shader if often only a small part of the original code is executed.
\paragraph{Background} Is there something? Maybe for CPUs?
\paragraph{Example} Blend multiple textures based on a material texture which most often contains a single 1 component.
	
\subsubsection{Use the scalar processor to fetch memory if all SIMD-threads want to fetch the same address}
\paragraph{Needed data} Address values per wave
\paragraph{Description} If every SIMD-thread wants to fetch the same memory, the code can be optimized to fetch the address once by the scalar processor instead of the vector processors. This reduces the latency(?).
\paragraph{Background}
\paragraph{Example}

\subsection{Notation}
\label{sub:notation}
Best notation for (SIMD-)threads, \glspl{cu}.

\subsection{Contributions}
\label{sub:contributions}
