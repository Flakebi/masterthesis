\clearpage
\bigsection{Evaluation}
To evaluate our work, we run our analyses on various applications and also run benchmarks comparing the performance with and without profile-guided optimizations.
We modified the Vulkan driver to enable PGO so we need Vulkan applications for benchmarks.
We picked the following five games which have in common that they use the Vulkan API on Linux, and they have a benchmark which can be executed automatically and collects performance data in a machine-readable form:
Ashes of the Singularity which is abbreviate as \emph{Ashes} in the following, Dota 2, F1 2017, Mad Max and Warhammer 40,000: Dawn of War III which we shorten to Warhammer.
There are similar games, which we tried but failed because of various problems like the game does not start at all under Linux.
The games that we excluded are F1 2018, Serious Sam Fusion 2017 and Total War Saga: Thrones of Britannia, which was not even installable through Steam.

Another game focused application which we looked at is the Infiltrator Demo Project of the Unreal Engine~\cite{Games2015}.
This is not a playable game, but a demo scene which plays when the application starts and which showcases the game engine.
Also, it does not measure the performance, so we did not use it to test the performance of PGO, but we measured the number of needed registers.
Another non-game application which we used for evaluation purpose is a small switch-case bytecode interpreter that we describe further in \cref{sub:eval_unused}.
In total, more than $6 \frac{1}{2}\;\si{h}$ was spent running benchmarks.

The performance numbers are expressed as frame times, i.e. how long does it take to compute a single frame.
For example, a frame time of \SI{16.6}{\milli\second} corresponds to 60 frames per second.
The lower the frame time is, the more pictures are shown per second and the better is the performance.
The measured times are always given with a standard error using a confidence level of \SI{68.3}{\percent} with the Student's t-distribution.

An application like a game uses a lot of shaders, Ashes has 250 different shaders, Dota 2 compiles 877 shaders for the benchmark scene and Mad Max 734 shaders.
Warhammer uses the least amount with 225 shaders while F1 2017 needs the most with 3220.
These numbers do not necessarily represent the total number of shaders in the games, but only the amount of shaders that are compiled for the benchmark scene.
A difference here is that Ashes compiles most of its shaders at the start of the game while Dota compiles shaders lazily when they are needed, so we only see a fraction of the shaders that Dota contains, but we see a greater fraction of the shaders in Ashes.
When evaluating basic block counters, unused code, and uniformity, we only consider shaders that were run at least once.
For the register usage, we collect the statistics overall compiled shaders without taking into account if they were executed or not.

\subsection{Basic Block Counters}
\label{sub:eval_counters}
Shaders are small programs compared to CPU programs, more like a single function.
This means the number of basic blocks in a shader is low, usually around 20 or lower.
After instrumenting shaders and running a benchmark, the compiler can associate a counter with each basic block is.

A performance comparison using the default optimizations in LLVM that rely on PGO data is discussed in \cref{sub:eval_perf}.
When instrumenting before the \texttt{StructurizeCFG} pass, an effect of PGO is that most pixel shaders are marked as hot functions while vertex shaders are often marked as unlikely.
The reason is that a triangle contains three vertices but a lot more pixels.
Hence a pixel shader gets executed a lot more often than a vertex shader and LLVM marks them accordingly.

To get an intuition for the collected counter values, we visualize them in a few diagrams.
\Cref{dia:counter_values} shows a histogram of the basic blocks, sorted by execution count.
The x-axis of this diagram represents the counter, i.e. how often a basic block is executed.
A bar for the execution counts $0.8\cdot 10^{12}$--$1.2\cdot 10^{12}$, for example, represents all blocks which get run at least $0.8\cdot 10^{12}$ times but less than $1.2\cdot 10^{12}$ times.
The height of the bar counts how many basic blocks fall into this range, in this example one basic block of all shaders in Dota 2 falls into the range.
The lowest bucket of the diagram, the leftmost bar, contains most of the blocks.
This bar is cut off, otherwise, the rest of the bars would not be visible.
This means most basic blocks get executed less than $10^{10}$ times.
A few blocks run more often, two of them more than $10^{12}$ times.
The blocks that are run most often are the inner parts of a hot loop.
\input{figures/counter-values.tex}

\Cref{dia:counter_values2} gives a detailed view of the leftmost part of \cref{dia:counter_values}.
It only shows the range up to $10^5$ executions.
In this range, the blocks are distributed more equal, still, we see that most blocks are executed seldom while blocks with a higher counter value are rare.
\input{figures/counter-values2.tex}

Ashes as of 2019-09-01 shows graphical artifacts when using PGO.
This hints that there is a bug in LLVM that only gets triggered when profile-guided optimizations are enabled.
So far, we did not find the cause for this behavior.
In previous tests with PGO, Ashes did not show these artifacts, so maybe this bug is connected with a recent update of Ashes.

\subsection{Unused code}
\label{sub:eval_unused}
Looking at basic blocks that are never executed, the compiler can remove them and study the changes in register usage and performance.
The performance comparison can be found in \cref{sub:eval_perf}, this section shows statistics about the amount of unused code.
The fraction of unused basic blocks ranges from \SI{0.22}{\percent} in Mad Max up to \SI{37.46}{\percent} unused blocks in in Warhammer.
\Cref{dia:unused_by_game} shows a comparison of all analyzed games.
\input{figures/unused-by-game.tex}

To gain more insight into how shaders and the unused code is distributed, \cref{dia:unused_by_bb_ashes} and following diagrams show the unused code concerning the size of a shader in basic blocks.
The x-axis represents the size of a shader, the number of blocks that a shader contains.
The gray plot in the background displays the distribution of all shaders in an application.
A height of \SI{6}{\percent} at one point (the right y-axis is the crucial one here) means that about \SI{6}{\percent} of all shaders in the game have this number of basic blocks.
Each black line represents a shader with unused code. The left y-axis is connected to the percentage of unused code.
\input{figures/unused-by-bb.tex}

We test the effect of removing unused code on the games Dota 2 and Mad Max and on a small sample program.
Our code to remove basic blocks is not robust enough to work for all tested games and only Dota 2 and Mad Max ran flawlessly, thus the other games were not benchmarked in this configuration.

The small sample program which we tested in addition to the games runs a bytecode virtual machine in a shader using one large switch-case statement.
Each case statement contains the code for one virtual instruction and the shader iterates through every instruction.
The bytecode in our benchmark runs uses only a single instruction out of the 71 possible instructions.
The code is transferred to the shader through a uniform buffer, consists of 128 instructions stored in \SI{4}{\kibi\byte} of memory and is executed two times until the result is used as the final fragment color.
LLVM compiles the switch statement into a binary jump tree, therefore in each iteration of the virtual machine, multiple jumps are executed until the right case for the current instruction is found.

Using PGO, the compiler detects that the instructions in all but one case statements are never executed.
Thus, in the optimized version all these basic blocks are gone.
This leaves us with the compare instructions from the original binary jump tree but the branch instructions are removed and the code has fewer basic blocks than before.
This improves the performance as shown in \cref{sub:eval_perf}.

Apart from performance, we also want to analyze the register usage of shaders when removing unused blocks.
This can help us to estimate gains of register spilling to reduce register pressure.
When looking at registers, we have to differentiate between scalar and vector registers.
Scalar registers are cheap in hardware compared to vector registers because only $\frac{1}{64}$ of them is needed.
Hence, the hardware has more of them and the occupancy of shaders is mostly limited by vector registers.
We also split our observations by shader type, we look at compute, vertex and pixel/fragment shaders separately.
Each game is tested in three configurations: Running the game without any special options, using profile-guided optimizations and using PGO with removing unused basic blocks.
The figures starting at \cref{dia:registers_cs-vgpr} show the register usage, averaged over all shaders that fall into the category, e.g. vector registers in compute shaders in the first diagram.
The register usage for our switch vm is not shown because it stays unchanged, only one vector register less is used and the occupancy does not change.
\input{figures/registers.tex}

The average register usage stays the same when using PGO. Ashes sometimes uses a bit fewer registers than normal, F1 2017 sometimes more but the differences are small.
Mad Max shows the greatest effect when removing unused code, it sometimes reduces the register usage by one register on average.
A more detailed analysis of the effect of removing unused blocks in \cref{dia:registers_scatter} shows the register amount for each shader before and after removing blocks.
Every dot represents a shader, the x-coordinate is the number of used registers when compiling the shader with PGO, the y-coordinate are the registers when compiling with PGO and additionally removing blocks which never get executed.
Most shaders lie on the diagonal, which means the number of registers does not change.
A few points are above the diagonal, so the compiler allocates more registers for them when unused code is removed.
Some points are also below the diagonal, meaning they need fewer registers when blocks are removed.
\input{figures/registers-scatter.tex}

Mad Max uses a pixel shader with a lot of computations, but these computations never get executed in the benchmarks.
Thus, when removing unused basic blocks, only a simple shader that checks an always false condition remains and fewer vector registers are used.
This shader can be seen in the lower right corner of \cref{dia:registers_scatter}.
Surprisingly, Dota 2 does not show a great effect when removing code, although nearly \SI{20}{\percent} of the basic blocks are removed.

In summary, removing unused code can greatly reduce the register usage for some shaders.
Unused code, in general, does not seem to be a problem for games, at least not from the perspective of register usage.

\subsection{Uniform Branches}
\label{sub:eval_uniform_branches}
Apart from basic block counters, we examined the uniformity of certain variables.
One type of variables that we further looked at is branch conditions.
If a condition is uniform, the branch which is guarded by this condition is taken uniformly by the SIMD unit, if the condition is divergent, the unit has to take both branches after each other.

At compile time, a variable can be classified as uniform or as divergent.
If it is classified as uniform, the compiler knows that under all possible circumstances, the variable always contains the same value across all lanes.
Usually, such variables are stored in scalar registers and computations on these variables use the scalar unit.
If a variable is classified as divergent, the compiler is not able to prove that the variable always contains the same value on all lanes.
It does not necessarily mean that the variable value differs on multiple lanes at runtime.
Our instrumentation records the behavior at runtime and classifies a variable as \emph{dynamic uniform} if it always contains the same value on all lanes or as divergent if not.
\emph{Static uniform} variables are the ones where the compiler can prove that they are uniform.

\input{figures/uniform-branches.tex}

\Cref{dia:uniform_branches} shows how conditions and branches are divided into three categories.
The \enquote{best} category is the static uniform branches.
For these branches, the compiler can emit a simple branch instruction and the compiler knows that either all or none of the active lanes will take this branch.
Conditions in the dynamic uniform category represent a branch where the compiler emitted additional code to support divergent branching.
At runtime, this code is not used, the branch is always taken by all lanes uniformly.
The performance wise worst case are divergent branches.
In these cases, the compiler emits additional code like in the dynamic uniform case and at runtime, both branches are taken by different lanes, so the SIMD unit needs to execute both parts sequentially.

With the grading of the last paragraph, Ashes and F1 2017 are performing best because most of their branches are statically uniform and only a small fraction is divergent.
We need to take into account though that this is a very coarse look at these games, we do not know the size and performance impact of the divergent branches, also sometimes divergent branches are inevitable.

In general, static uniform branches form the biggest group, followed by dynamic uniform and divergent branches.
It depends on the game if the statically divergent branches are mostly uniform or divergent at runtime.
In F1 2017 most are uniform and the minority is divergent while in Mad Max it is the other way around.

\subsection{Uniform Loads}
\label{sub:eval_uniform_loads}
Similar to the analysis of uniform conditions in the last section, we analyzed \texttt{LoadInstr} instructions in LLVM.
The uniformity of loaded memory values is shown in \cref{dia:uniform_loads}.
It is important to know that the analyzed load instructions do not include buffer loads and image loads which are more complicated to analyze because they are indexed with multiple dimensions, can return multiple dimensions, are connected with a sampler, use mipmapping, etc.
\input{figures/uniform-loads.tex}

From the instrumented loads, divergent loads are the smallest category, staying under \SI{35}{\percent} in every game.
For Ashes and Warhammer, dynamic uniform loads are the common case. For the rest of the games, static uniform loads make up more than half of the loads, leading to different distributions of load uniformity in ever game.

\subsection{Performance}
\label{sub:eval_perf}
In addition to the previous analyses, an important part is the performance change when using profile-guided optimizations.
To evaluate the performance of shaders, we run the games in three different configurations: Without any special options, using profile-guided optimizations and using PGO with removing unused basic blocks.
As before, removing blocks is only done for Dota 2, Mad Max and the switch vm, the other games are not working with the current implementation.
\input{figures/performance.tex}

\begin{table}
	\centering
	\runtex{performance}
	\captionof{table}{Performance of games with PGO}
	\label{tab:performance}
\end{table}

The result is displayed in \cref{dia:performance} and \cref{tab:performance}.
In general, the optimizations did not change the performance of games by a great amount.
On Ashes and F1 2017 it even had a negative effect, these games got a little slower.
Dota and Mad Max did not show a significant difference.
Warhammer reduced the time per frame from \SI{65.98 \pm 0.18}{\milli\second} to \SI{65.31 \pm 0.13}{\milli\second} with PGO, which is an improvement of \SI{1 \pm 0.4}{\percent}.
The switch vm shows big improvements. When using PGO, it gets faster by \SI{7.34 \pm 0.10}{\percent}.
When additionally removing unused blocks, it gets even faster by \SI{21.06 \pm 0.10}{\percent}.

The order of basic blocks and other optimizations based on basic block counter seem to have a minor effect on the performance of games.
Removing unused blocks did not affect the two tested games.
On the switch vm however, PGO as well as removing basic blocks had a remarkable effect.

\subsection{Overhead}
\label{sub:overhead}
To find hot paths, the instrumentation inserts counters into some basic blocks of a program. The counters introduce an overhead, compared to a non-instrumented version of the code.
In the case of counting the frequency of basic block executions, the counters themselves are not sensitive to timing and thus not directly influenced by this overhead.
But the code will run slower. In the case of a game, there will be fewer frames per second than usual.
If the benchmark, that is run with PGO instrumentation, runs for a fixed time, the basic block frequencies will be lower.
Therefore, the measured counters may show fewer executions than the actual frequencies that should be obtained.
This effect should not have an impact on the optimization results because the ratio of frequencies stays the same, no matter how fast or slow a game runs.

We measured the overhead with two configurations.
The baseline in \cref{dia:overhead} and \cref{tab:overhead} is a normal run of the game with no instrumentation.
The first tested configuration uses atomic add instructions to increment the counters.
The second variant uses normal add instructions, i.e. no atomics.
All instrumentation was inserted after structurizing the CFG and increments the counter by one for the whole unit.
\input{figures/overhead.tex}

\begin{table}
	\centering
	\runtex{overhead}
	\captionof{table}{Overhead of BB counters}
	\label{tab:overhead}
\end{table}

The benchmarks show that an atomic add causes less overhead than the non-atomic version.
The reason is that the non-atomic increment needs a load and a store operation where the atomic version only needs a single memory transaction. The actual addition is computed inside the L2 cache for atomics.
Similar to the PGO benchmarks, Ashes shows the smallest difference.
The overhead of atomic counter instrumentation ranges from \SI{0.010 \pm 0.013}{\percent} for Ashes to \SI{43.1 \pm 1.9}{\percent} for Warhammer.
The average overhead for the five games is at \SI{16}{\percent}, the average for the non-atomic version at \SI{42}{\percent}.

In some cases, it may be necessary to decrease the overhead of basic block counters. For example to use the instrumentation in production environments where high overheads are not acceptable.
Sometimes, the overhead is high enough to trigger timeouts in the GPU driver, leading to game crashes. This is the case for the Infiltrator demo project of the Unreal Engine when using atomic counters with one atomic operation per lane.

A simple way to decrease the overhead is to aggregate the counter addition to be executed once per SIMD unit instead of once per lane.
I.e. for per unit counting, all lanes but one are turned off when the counter is incremented.
For per lane counting, the same happens but the number of previously active lanes is added to the counter.
This is enough to let e.g. the Infiltrator demo start.
All our benchmarks used this technique.

Another overhead reduction can be achieved by skipping counting on most SIMD units and activate it e.g. only on \SI{5}{\percent} of the units.
This means most executions will skip the increments, having even less overhead than the non-atomic variant while the rest of the executions gives accurate statistics.
A problem with this approach is that (e.g. vertex-) shaders that get executed only a few times, might not land on any of SIMD units where counting is activated and no statistics at all are collected for them.
This should only be a small problem because these shaders probably do not account for much of the computation time (as they are executed only a few times) and optimizing them cannot yield big benefits anyway.

A more sophisticated technique that can speed up basic block counting tries to reduce the memory pressure by atomics without falling back to non-atomic counting.
As we know, many atomic operations will simultaneously try to access the same memory location.
We can reduce this pressure if multiple memory locations in different cache lanes are used for the same counter and they spread the counting over these locations.
As fewer atomic increments access the same memory, they do not have to wait as long as before and the program execution can continue faster.
In the end, when shutting down the application, the driver has to add up all duplicated counters.
This happens only once in the end for most applications so it adds not much overhead.
