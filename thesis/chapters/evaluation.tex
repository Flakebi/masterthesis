\clearpage
\bigsection{Evaluation}
To evaluate our work, we run our analyses on various applications and also run benchmarks to compare the performance before and after using profile-guided optimizations.
As we need Vulkan applications, we picked the following five games:
Ashes of the Singularity which we will sometimes abbreviate as \emph{Ashes} in the following, Dota 2, F1 2017, Mad Max and Warhammer 40,000: Dawn of War III which we will shorten to Warhammer.
We also looked at Vulkan benchmarks like vkmark, but this tool does not focus on shaders but instead on the rest of the graphics pipeline.
Therefore, the shaders are simple and do not provide possibilities for PGO. E.g. using the default LLVM PGO on the shaders did not change a single instruction.
The selected games have in common that they use the Vulkan API on Linux, and they have a benchmark which can be executed automatically and collects the performance data in a machine-readable form.
There are similar games, which we tried to use but failed because of various problems, like the game does not start at all under Linux.
The games that we excluded are F1 2018, Serious Sam Fusion 2017 and Total War Saga: Thrones of Britannia, which was not even installable through Steam.

Another game focused application which we looked at is the Infiltrator Demo Project of the Unreal Engine~\cite{Games2015}.
This is not a real game, but a demo scene is played when the application starts to showcase the game engine.
Also, it does not measure the performance to our knowledge, so we did not use it to test the performance of PGO, but we measured the amount of needed registers.
Another non-game application which we used for evaluation purpose is a small switch-case bytecode interpreter that we will describe further in \cref{sub:eval_unused}.
In total, more than $6 \frac{1}{2}\;\si{h}$ was spent running benchmarks.

The performance numbers are expressed as frame times, i.e. how long does it take to compute a single frame for the game.
For example a frame time of \SI{16.6}{\milli\second} roughly corresponds to 60 frames per second.
The lower the frame time is, the more pictures are shown per second and the better is the performance.
The measured times are always given with a standard error using a confidence level of \SI{68.3}{\percent} with the Student's t-distribution.

An application like a game uses a lot of shaders, Ashes has 250 different shaders, Dota 2 compiles 877 shaders for the benchmark scene and Mad Max 734 shaders.
Warhammer uses the least amount with 225 shaders while F1 2017 needs the most with 3220.
These numbers do not necessarily represent the total number of shaders in the games, but only the amount of shaders which are compiled for the benchmark scene.
A difference here is that Ashes compiles most of its shaders at the start of the game while Dota compiles shaders lazily when they are needed, so we only saw a fraction of the shaders that Dota contains, but we saw a greater fraction of the shaders in Ashes.
When evaluating basic block counters, unused code and uniformity, we only consider shaders which were run at least once.
For the register usage, we collect the statistics over all compiled shaders without taking into account if they have been needed or not.

\subsection{Basic Block Counters}
\label{sub:eval_counters}
Shaders are small programs compared to CPU programs, more like a single function.
This means the number of basic blocks in a shader is low, usually around 20 or lower.
After instrumenting the shaders and running a benchmark, we have a counter associated with each basic block which tells us how often each block was executed.

LLVM uses the counters e.g. for linearization. A performance comparison using the default optimizations in LLVM that rely on PGO data is discussed in \cref{sub:eval_perf}.
An additional effect of PGO apart from linearization which can be observed when instrumenting before the \texttt{StructurizeCFG} pass is that most pixel shaders are marked as hot functions while vertex shaders are often marked as unlikely.
The reason is that a triangle contains three vertices but a lot more pixels.
Hence a pixel shader gets executed more often than a vertex shader and LLVM marks them accordingly.

To get an intuition for the collected counter values, we visualize them in a few diagrams.
\Cref{dia:counter_values} shows a histogram of the basic blocks, sorted by execution count.
The x-axis of this diagram represents the counter, i.e. how often a basic block is executed.
A bar for the execution counts $0.8\cdot 10^{12}--1.2\cdot 10^{12}$ for example represents all blocks which get run at least $0.8\cdot 10^{12}$ times but less than $1.2\cdot 10^{12}$ times.
The height of the bar counts how many basic blocks fall into this range, in this example one basic block of all shaders in Dota 2 falls into the range.
The lowest bucket of the diagram, the leftmost bar, contains most of the blocks.
This bar is cut off, otherwise the rest of the bars would not be visible.
This means most basic blocks get executed less than $10^10$ times.
A few blocks run a lot more often, two of them more than $10^{12}$ times.
The blocks that are run most often are the inner parts of a hot loop in a shader.
\input{figures/counter-values.tex}

\Cref{dia:counter_values2} gives a detailed view of the leftmost part of \cref{dia:counter_values}.
It only shows the range up to $10^5$ executions.
In this range, the blocks are distributed more equal, still we see that most blocks are executed seldom while blocks with a higher counter value are rare.
\input{figures/counter-values2.tex}

\subsection{Unused code}
\label{sub:eval_unused}
Looking at basic blocks which get never executed, we can remove them and study the changes in register usage and performance.
The performance comparison can be found in \cref{sub:eval_perf}, this section shows statistics about the amount of unused code.
The fraction of unused basic blocks ranges from \SI{0.22}{\percent} in Mad Max up to \SI{37.46}{\percent} unused blocks in in Warhammer.
\Cref{dia:unused_by_game} shows a comparison of all analyzed games.
\input{figures/unused-by-game.tex}

To gain more insight into how shaders and the unused code is distributed, \Cref{dia:unused_by_bb_ashes} and following show the unused code with respect to the size of a shader in basic blocks.
The x-axis represents the size of a shader, the amount of blocks that a shader contains.
The gray plot in the background displays the distribution of all shaders in an application.
A height of \SI{6}{\percent} at one point (the right y-axis is the crucial one here) means that about \SI{6}{\percent} of all shaders in the game have this amount of basic blocks.
Each black line represents a shader with unused code. The left y-axis is connected to the percentage of unused code.
\input{figures/unused-by-bb.tex}

We test the effect of removing unused code on the games Dota 2 and Mad Max and on a small sample program.
Our code to remove basic blocks is not robust enough to work for all tested games and only Dota 2 and Mad Max ran flawlessly, thus the other games were not benchmarked in this configuration.
The small sample program runs a bytecode virtual machine in a shader using one large switch-case statement.
Each case statement contains the code for one virtual instruction and the shader iterates through every instruction.
The bytecode in our benchmark runs uses only a single instruction out of the 71 possible instructions.
The code is transferred to the shader through a uniform buffer, consists of 128 instructions stored in \SI{4}{\kibi\byte} of memory and is executed two times until the result is used as the final fragment color.
LLVM compiles the switch statement into a binary jump tree, therefore in each iteration of the virtual machine, multiple jumps are executed until the right case for the current instruction is found.

Using PGO, we detect that all instructions in all but one case statements are never executed.
Thus, in the optimized shader version we can remove all these basic blocks.
This leaves us with compare and jump instructions from the original binary jump tree but only one of the branch instructions is taken and the code has a lot less basic blocks than before.
This improves the performance as seen in \cref{sub:eval_perf}.

Apart from performance, we also want to analyze the register usage of shaders when removing unused blocks.
This can help us to estimate gains of register spilling to reduce register pressure.
When looking at registers, we have to differentiate between scalar and vector registers.
Scalar registers are cheap in hardware compared to vector registers because we only need $\frac{1}{64}$ of them.
Hence, we have more of them and the occupancy of shaders is mostly limited by vector registers.
We also split our observations by shader type, we look at compute, vertex and pixel/fragment shaders separately.
Each game is tested in three configurations: Running the game without any special options, using profile-guided optimizations and using PGO with removing unused basic blocks.
The figures starting at \cref{dia:registers_cs-vgpr} show the register usage, averaged over all shaders that fall into the category, e.g. vector registers in compute shaders in the first diagram.
\input{figures/registers.tex}

Mad Max uses a pixel shader with a lot of computations, but these computations never get executed in the benchmarks.
Thus, when removing unused basic blocks, only a simple shader that checks an --- always false --- condition remains and a lot less vector registers are used.

\input{figures/registers-scatter.tex}

\subsection{Uniform Branches}
\label{sub:eval_uniform_branches}
Uniformity of conditions

\input{figures/uniform-branches.tex}

\subsection{Uniform Loads}
\label{sub:eval_uniform_loads}

\input{figures/uniform-loads.tex}

\subsection{Performance}
\label{sub:eval_perf}
Performance comparison in three configurations: Running the game without any special options, using profile-guided optimizations and using PGO with removing unused basic blocks.

The optimizations do not change the performance of the selected games by much.
The order of basic blocks seems to have not a big impact on the performance.
Some games, Ashes and F1 2017, are a bit slower when PGO is active while Warhammer and Mad Max are faster.
Dota 2 does not have a significant change in performance.

\input{figures/performance.tex}

\begin{table}
	\centering
	\runtex{performance}
	\captionof{table}{Performance of games with PGO}
	\label{tab:performance}
\end{table}

\subsection{Overhead}
\label{sub:overhead}
To find hot paths, the instrumentation inserts counters into some basic blocks of a program. The counters introduce an overhead, compared to a non-instrumented version of the code.
In the case of counting the frequency of basic block executions, the counters themselves are not sensitive to timing and thus not directly influenced by this overhead.
But the code will run slower. In the case of a game, we will observe fewer frames per second than usual.
If the benchmark, that we run with PGO instrumentation, runs for a fixed time, we will get lower basic block frequencies for fewer frames per second.
Therefore, the measured counters may be less than the actual frequencies that we try to measure.

We measured the overhead for two different games in different configurations: Dota 2 and Ashes of the Singularity. The baseline in \cref{dia:overhead} and \cref{tab:overhead} is a normal run of the game with no counters.
The first tested configuration uses normal add instructions (i.e. no atomics) and increments by one per SIMD unit.
The second and third variant use atomic counters, either incrementing by one on each SIMD lane or once per unit.
All instrumentation was inserted after structurizing the CFG.

As expected, incrementing per lane needs the most time. An atomic add causes less overhead for Dota 2 than the non-atomic version.
The reason is that the non-atomic increment needs a load and a store operation where the atomic version needs only a single memory transaction. The actual addition is computed inside the L2 cache for atomics.
For Ashes of the Singularity, we cannot see much of a difference in performance. The per-lane variant is significantly slower but only by \SI{0.25 \pm 0.06}{\percent}.

\input{figures/overhead.tex}

Similar to the PGO benchmarks, Ashes shows the smallest difference.

\begin{table}
\centering
\runtex{overhead}
\captionof{table}{Overhead of BB counters}
\label{tab:overhead}
\end{table}


In some cases, it may be necessary to decrease the overhead of basic block counters. For example to use the instrumentation in production environments where high overheads are not acceptable.
Sometimes, the overhead is high enough to trigger timeouts in the GPU driver, leading to game crashes. This is the case for the Infiltrator demo project of the Unreal Engine.

A simple way to decrease the overhead is to switch from atomic counters to non-atomic counting. This is enough to let e.g. the Infiltrator demo start, however it comes with a drawback. Shaders are executed highly parallel, so many instances of a shader will simultaneously access the same memory location of a counter.
These race conditions lead to inaccurate counters. Not only do we get wrong proportions of frequencies, we also have to take into account that not every basic block gets a counter, some frequencies are computed from multiple counters. This leads to cases where basic blocks that are not executed at all at runtime are assigned a counter value of several thousand executions because the counter values used to compute the frequencies are inaccurate.

Another overhead reduction can be achieved by skipping counting on most SIMD units and activate it e.g. only on \SI{5}{\percent} of the units.
This means most executions will skip the increments, having even less overhead than the non-atomic variant while the rest of the executions gives accurate statistics.
A problem with this approach is that (e.g. vertex-) shaders that get executed only a few times, might not land on any of SIMD units where counting is activated and we get no statistics at all for them.
This should only be a small problem because these shaders probably do not account for much of the computation time (as they are executed only a few times) and optimizing them cannot yield big benefits anyway.

A more sophisticated technique that can speed up basic block counting tries to reduce the memory pressure by atomics without falling back to non-atomic counting. As we know, many atomic operations will simultaneously try to access the same memory location.
We can reduce this pressure if we add multiple memory locations in different cache lanes for the same counter and spread the counting over these locations. As less atomic increments access the same memory, they do not have to wait as long as before and the program execution can continue faster.
In the end, when shutting down the application, we have to add up all duplicated counters. This happens only once in the end for most applications so it adds not much overhead.
