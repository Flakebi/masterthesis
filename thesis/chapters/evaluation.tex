\clearpage
\bigsection{Evaluation}
% TODO How much time spend benchmarking? + 45 + 1:40 + 25 + 1:48 + 21 + 38
To evaluate our work, we run the analyses on various applications and also run benchmarks to compare the performance before and after using profile-guided optimizations.
As we need Vulkan applications, we picked the following five games:
Ashes of the Singularity which we will sometimes abbreviate as \emph{Ashes} in the following, Dota 2, F1 2017, Mad Max and Warhammer 40,000: Dawn of War III which we will shorten to Warhammer.
We also looked at Vulkan benchmarks like vkmark, but this tool does not focus on shaders but instead on the rest of the graphics pipeline.
Therefore, the shaders are simple and do not provide possibilities for PGO. E.g. using the default LLVM PGO on the shaders did not change a single instruction.

The performance numbers are mostly expressed as frame times, i.e. how long does it take to compute a single frame for the game.
For example a frame time of \SI{16.6}{\milli\second} roughly corresponds to 60 frames per second.
The measured times are always given with an error.

\subsection{Basic Block Counters}
\label{sub:eval_counters}
Shaders are small programs compared to CPU programs, more like a single function. This means the number of basic blocks in a shader is low as seen in \cref{dia:shader_bbs_dota}.
LLVM uses the counters e.g. for linearization. A performance comparison using the default optimizations in LLVM that rely on PGO data is shown in \cref{dia:performance}.
% TODO
The optimizations do not change the performance of the selected games significantly.
The order of basic blocks seems to have no significant impact on the performance.

An additional effect which can be observed when compiling shaders with PGO and using instrumentation before the \texttt{StructurizeCFG} pass is that most pixel shaders are marked as hot functions while vertex shaders are often marked as unlikely.
The reason is that for every three vertices of a triangle, a lot more pixels are drawn.
Hence a pixel shader gets executed a lot more often than a vertex shader and LLVM marks them accordingly.

To get an intuition for the counter values we get, \cref{dia:counter_values} displays a histogram of the basic blocks, sorted by execution count.
The x-axis of this diagram represents the counter, i.e. how often a basic block is executed.
A bar for the execution counts 20--30 for example represents all blocks which get run at least 20 times but at max 30 times (excluded).
The high of the bar counts how many basic blocks fall into this range.
Lots of blocks fall into the lowest bucket of the diagram, the leftmost bar. This bar is cut off, otherwise the rest of the bars would not be visible.
This means most basic blocks get executed less than $10^6$ times. A few blocks run a lot more often, more than $10^12$ times.

\Cref{dia:counter_values2} gives a detailed view of the leftmost part of \cref{dia:counter_values}.

\input{figures/counter-values.tex}
\input{figures/counter-values2.tex}

\subsection{Unused code}
\label{sub:eval_unused}
We test the effect of removing unused code with a small sample program which runs a bytecode virtual machine in a shader with one large switch-case statement.
The bytecode computes the final fragment color and in our example only uses a single bytecode instruction out of 71 possible instructions.
Using PGO, we detect that all other instructions are never executed and in the optimized shader version we remove all these basic blocks.
This leaves us with compare and jump instructions from the original binary jump tree but only one of the branch instructions is taken and the code has a lot less basic blocks than before.

\input{figures/unused-by-bb.tex}
\input{figures/unused-by-game.tex}

Register usage, split by shader type.
For each game in three configurations: Running the game without any special options, using profile-guided optimizations and using PGO with removing unused basic blocks.
\input{figures/registers-cs.tex}
\input{figures/registers-vs.tex}
\input{figures/registers-fs.tex}

% TODO Scatter plot

\subsection{Uniform Branches}
\label{sub:eval_uniform_branches}

\input{figures/uniform-branches.tex}

\subsection{Uniform Loads}
\label{sub:eval_uniform_loads}

\input{figures/uniform-loads.tex}

\subsection{Performance}
\label{sub:eval_perf}
Performance comparison in three configurations: Running the game without any special options, using profile-guided optimizations and using PGO with removing unused basic blocks.

\input{figures/performance.tex}

\subsection{Overhead}
\label{sub:overhead}
To find hot paths, the instrumentation inserts counters into some basic blocks of a program. The counters introduce an overhead, compared to a non-instrumented version of the code.
In the case of counting the frequency of basic block executions, the counters themselves are not sensitive to timing and thus not directly influenced by this overhead.
But the code will run slower. In the case of a game, we will observe fewer frames per second than usual.
If the benchmark, that we run with PGO instrumentation, runs for a fixed time, we will get lower basic block frequencies for fewer frames per second.
Therefore, the measured counters may be less than the actual frequencies that we try to measure.

We measured the overhead for two different games in different configurations: Dota 2 and Ashes of the Singularity. The baseline in \cref{dia:overhead} and \cref{tab:overhead} is a normal run of the game with no counters.
The first tested configuration uses normal add instructions (i.e. no atomics) and increments by one per SIMD unit.
The second and third variant use atomic counters, either incrementing by one on each SIMD lane or once per unit.
All instrumentation was inserted after structurizing the CFG.

As expected, incrementing per lane needs the most time. An atomic add causes less overhead for Dota 2 than the non-atomic version.
The reason is that the non-atomic increment needs a load and a store operation where the atomic version needs only a single memory transaction. The actual addition is computed inside the L2 cache for atomics.
For Ashes of the Singularity, we cannot see much of a difference in performance. The per-lane variant is significantly slower but only by \SI{0.25 \pm 0.06}{\percent}.

\input{figures/overhead.tex}

% TODO Why ashes? Code size / BB-count

\begin{table}
\centering
\runtex{overhead}
\captionof{table}{Overhead of BB counters}
\label{tab:overhead}
\end{table}


In some cases, it may be necessary to decrease the overhead of basic block counters. For example to use the instrumentation in production environments where high overheads are not acceptable.
Sometimes, the overhead is high enough to trigger timeouts in the GPU driver, leading to game crashes. This is the case for the Infiltrator demo project of the Unreal Engine.

A simple way to decrease the overhead is to switch from atomic counters to non-atomic counting. This is enough to let e.g. the Infiltrator demo start, however it comes with a drawback. Shaders are executed highly parallel, so many instances of a shader will simultaneously access the same memory location of a counter.
These race conditions lead to inaccurate counters. Not only do we get wrong proportions of frequencies, we also have to take into account that not every basic block gets a counter, some frequencies are computed from multiple counters. This leads to cases where basic blocks that are not executed at all at runtime are assigned a counter value of several thousand executions because the counter values used to compute the frequencies are inaccurate.

Another overhead reduction can be achieved by skipping counting on most SIMD units and activate it e.g. only on \SI{5}{\percent} of the units.
This means most executions will skip the increments, having even less overhead than the non-atomic variant while the rest of the executions gives accurate statistics.
A problem with this approach is that (e.g. vertex-) shaders that get executed only a few times, might not land on any of SIMD units where counting is activated and we get no statistics at all for them.
This should only be a small problem because these shaders probably do not account for much of the computation time (as they are executed only a few times) and optimizing them cannot yield big benefits anyway.

A more sophisticated technique that can speed up basic block counting tries to reduce the memory pressure by atomics without falling back to non-atomic counting. As we know, many atomic operations will simultaneously try to access the same memory location.
We can reduce this pressure if we add multiple memory locations in different cache lanes for the same counter and spread the counting over these locations. As less atomic increments access the same memory, they do not have to wait as long as before and the program execution can continue faster.
In the end, when shutting down the application, we have to add up all duplicated counters. This happens only once in the end for most applications so it adds not much overhead.
