\clearpage
\bigsection{Evaluation}
Something like threads to validity: Inserting profiling code into a shader changes the behavior of the shader.
Impact of instrumentation makes it slower, however the rendered scenes stay the same, we may get smaller counter values than without instrumentation
It should have no impact when measuring branch probabilities, as the taken branches do not change when inserting the branch counting code \emph{after optimizations}.

\subsection{Effects}
\label{sub:effets}
The basic block counters influence order of basic blocks -> No impact on performance on most shaders?
Marks pixel shaders as hot and vertex shaders as unlikely -> No impact on performance.

\subsection{Basic Block Counters}
\label{sub:evaluation_counters}
% TODO diagram
Shaders are small programs compared to CPU programs, more like a single function. This means the number of basic blocks in a shader is low as seen in \cref{dia:shader_bbs_dota}.
LLVM uses the counters e.g. for linearization. A performance comparison using the default optimizations in LLVM that rely on PGO data is shown in \dots. The optimizations do not change the performance of the selected games significantly.

\begin{figure}
\pgfplotsset{width=\textwidth}
\centering
\begin{minipage}[t]{.45\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
	ybar,
	xlabel={\#BBs in a shader},
	ylabel={\#Shaders},
	xmin=0,
	xmax=40,
	grid=both,
	axis lines=left,
	bar width=0.5cm,
]

\addplot [
	fill=tumblue,
] table {data/dota_bbs.txt};
\end{axis}
\end{tikzpicture}
\captionof{figure}{The amount of basic blocks per shader in Dota 2}
\label{dia:shader_bbs_dota}
\end{minipage}\qquad
\begin{minipage}[t]{.45\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
	ybar,
	xlabel={\#BBs in a shader},
	ylabel={\#Shaders},
	xmin=0,
	xmax=60,
	grid=both,
	axis lines=left,
	bar width=0.3cm,
]

\addplot [
	fill=tumblue,
] table {data/ashes_bbs.txt};
\end{axis}
\end{tikzpicture}
\captionof{figure}{The amount of basic blocks per shader in Ashes of the Singularity}
\label{dia:shader_bbs_ashes}
\end{minipage}
\end{figure}

\subsection{Unused code}
\label{sub:evaluation_unused}
We test the effect of removing unused code with a small sample program which runs a bytecode virtual machine in a shader with one large switch-case statement.
The bytecode computes the final fragment color and in our example only uses a single bytecode instruction out of 71 possible instructions.
Using PGO, we detect that all other instructions are never executed and in the optimized shader version we remove all these basic blocks.
This leaves us with compare and jump instructions from the original binary jump tree but only one of the branch instructions is taken and the code has a lot less basic blocks than before.

\subsection{Overhead}
\label{sub:overhead}
To find hot paths, the instrumentation inserts counters into some basic blocks of a program. The counters introduce an overhead, compared to a non-instrumented version of the code.
In the case of counting the frequency of basic block executions, the counters themselves are not sensitive to timing and thus not directly influenced by this overhead.
But the code will run slower. In the case of a game, we will observe fewer frames per second than usual.
If the benchmark, that we run with PGO instrumentation, runs for a fixed time, we will get lower basic block frequencies for fewer frames per second.
Therefore, the measured counters may be less than the actual frequencies that we try to measure.

We measured the overhead for two different games in different configurations: Dota 2 and Ashes of the Singularity. The baseline in \cref{dia:overhead} and \cref{tab:overhead} is a normal run of the game with no counters.
The first tested configuration uses normal add instructions (i.e. no atomics) and increments by one per SIMD unit.
The second and third variant use atomic counters, either incrementing by one on each SIMD lane or once per unit.
All instrumentation was inserted after structurizing the CFG.

As expected, incrementing per lane needs the most time. An atomic add causes less overhead for Dota 2 than the non-atomic version.
The reason is that the non-atomic increment needs a load and a store operation where the atomic version needs only a single memory transaction. The actual addition is computed inside the L2 cache for atomics.
For Ashes of the Singularity, we cannot see much of a difference in performance. The per-lane variant is significantly slower but only by \SI{0.25 \pm 0.06}{\percent}.

\begin{figure}
\centering
\pgfplotsset{width=0.75\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
	ybar=0.2cm,
	enlarge x limits=0.4,
	symbolic x coords={dota,ashes},
	xticklabels={Dota 2,Ashes of the Singularity},
	xtick=data,
	ylabel={\footnotesize {\color{tumblue}$\blacktriangleleft$} Time per frame [\SI{}{\milli\second}], less is better},
	ymin=0,
	ymax=210,
	grid=both,
	nodes near coords,
	every node near coord/.append style={rotate=90, anchor=west},
	bar width=0.5cm,
	legend style={at={(1,1)},anchor=north east},
]

\addplot [
fill=color0,
error bars/.cd,
y dir=both,y explicit
] table[header=false, y error index=2] {data/overhead-none.txt};

\addplot [
	fill=color1,
	error bars/.cd,
y dir=both,y explicit
] table[header=false, y error index=2] {data/overhead-non-atomic-wave-late.txt};

\addplot [
	fill=color2,
	error bars/.cd,
y dir=both,y explicit
] table[header=false, y error index=2] {data/overhead-late.txt};

\addplot [
	fill=color3,
	error bars/.cd,
y dir=both,y explicit
] table[header=false, y error index=2] {data/overhead-wave-late.txt};

\legend{No counters,Non-atomic per SIMD unit,Atomic,Atomic per SIMD unit}
\end{axis}
\end{tikzpicture}
\captionof{figure}{Overhead of BB counters}
\label{dia:overhead}
\end{figure}
% TODO Why ashes? Code size / BB-count

\begin{figure}
\centering
\runtex{overhead}
\captionof{table}{Overhead of BB counters}
\label{tab:overhead}
\end{figure}


In some cases, it may be necessary to decrease the overhead of basic block counters. For example to use the instrumentation in production environments where high overheads are not acceptable.
Sometimes, the overhead is high enough to trigger timeouts in the GPU driver, leading to game crashes. This is the case for the Infiltrator demo project of the Unreal Engine.

A simple way to decrease the overhead is to switch from atomic counters to non-atomic counting. This is enough to let e.g. the Infiltrator demo start, however it comes with a drawback. Shaders are executed highly parallel, so many instances of a shader will simultaneously access the same memory location of a counter.
These race conditions lead to inaccurate counters. Not only do we get wrong proportions of frequencies, we also have to take into account that not every basic block gets a counter, some frequencies are computed from multiple counters. This leads to cases where basic blocks that are not executed at all at runtime are assigned a counter value of several thousand executions because the counter values used to compute the frequencies are inaccurate.

Another overhead reduction can be achieved by skipping counting on most SIMD units and activate it e.g. only on \SI{5}{\percent} of the units.
This means most executions will skip the increments, having even less overhead than the non-atomic variant while the rest of the executions gives accurate statistics.
A problem with this approach is that (e.g. vertex-) shaders that get executed only a few times, might not land on any of SIMD units where counting is activated and we get no statistics at all for them.
This should only be a small problem because these shaders probably do not account for much of the computation time (as they are executed only a few times) and optimizing them cannot yield big benefits anyway.

A more sophisticated technique that can speed up basic block counting tries to reduce the memory pressure by atomics without falling back to non-atomic counting. As we know, many atomic operations will simultaneously try to access the same memory location.
We can reduce this pressure if we add multiple memory locations in different cache lanes for the same counter and spread the counting over these locations. As less atomic increments access the same memory, they do not have to wait as long as before and the program execution can continue faster.
In the end, when shutting down the application, we have to add up all duplicated counters. This happens only once in the end for most applications so it adds not much overhead.
