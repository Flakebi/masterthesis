\clearpage
\bigsection{Background}
This section contains introductions into the hard- and software of graphics processors. Further we explain concepts that are needed for the implementation, like the ELF format.

\subsection{Graphics Processing Unit}
To understand how profiling data can -- and cannot -- be collected on GPUs, we need a basic understanding of how code is run on graphics processors. This section gives a short explanation of the underlying hardware and in which points it differs from CPUs.
Afterwards we take a closer look at the communication between CPU and GPU. This is important to collect runtime information about shaders because this is the part where we need to fetch and store collected analysis data. The hardware section is largely based on a blog series about compute shaders by Matth√§us Chajdas~\cite{Chajdas2018}.

The general concepts, approaches and results of this thesis should apply to all GPU vendors. For simplicity and because of the publicly available resources and code, we will focus the work on AMD graphics cards and their open source Vulkan driver AMDVLK.

To understand why GPUs are built the way they are, it is helpful to understand how developers write GPU code. Similar to CPU code, developers like to write simple code that does computations on a single data point. Much of the hard work like choosing instructions or parallelizing loops with \gls{simd} instructions can be done automatically by the compiler. This works similarly for GPU code. A developer writes code for e.g. computing the color of one pixel in a game. The compiler then chooses the right instructions and in the end the GPU does the computation for each pixel of a rendered triangle. The way \gls{simd} instructions are used on CPUs and GPUs is quite different though.

On a CPU, \gls{simd} instructions are often used to vectorize multiple iterations of a loop that are independent of each other. An equivalent way of expressing this would be to write SIMD instructions by hand. In summary, multiple instructions of the code a developer writes (which may be from multiple loop iterations) are fused into one SIMD instruction.

On a GPU however, loops are not vectorized. Instead, a program gets duplicated for multiple input data. For games this means the program is started multiple times -- one time per pixel -- to compute the color for every pixel. If the program flow is the same for all instances and only the input values are different, the GPU runs (almost) every instruction of a shader on a SIMD unit. The multiple input and output data of the SIMD unit are from multiple instances (pixels) of the shader. As we will see later, this poses a problem if the program flow is not the same. On CPUs this concept of parallelism is similar to different threads, which only have limited access to each other and run the same code on different data.

\paragraph{Hardware} This section describes the hardware architecture of AMD \gls{gcn} GPUs as this is the architecture we are most familiar with. The general concept of SIMD units is the same on NVIDIA graphic cards, though a difference is e.g. that there are 32 concurrent floating-point operations while there are 64 on AMD \gls{gcn} cards.

On the hardware level, the basic building block of a modern GPU is a big \gls{simd} unit. The same instruction is run on $16 \times$ 32-bit floating-point numbers simultaneously. We will call these 16 different numbers \emph{lanes}. For most instructions, the pipeline has 4 stages and an instruction takes 4 cycles to complete. This also means that after the first cycle, the GPU can issue a second instruction, computing on another 16 floating-point numbers. This could be the next instruction in the program but then the GPU would have to care about data dependencies between instructions. This would add a lot of complexity so AMD decided to do something else instead. They virtually enlarge the SIMD unit to work on a multiple of 16 numbers and execute the same instruction again. The GPU can execute the same instruction 4 times, then the first instruction will be finished in the next cycle. This means at one point in the pipeline, there are 4 instructions in flight, each computing on 16 numbers, which makes a total of 64 different lanes. For a developer, this looks like a SIMD unit which runs computations on 64 floats in parallel.

In addition to the SIMD unit, a \gls{gcn} GPU contains a few other processing units. One of them is the \emph{scalar unit}. This unit comes along the SIMD units, so a program that is run on the GPU contains both, vector instructions for the SIMD units and scalar instructions for the scalar unit in an interleaved manner. The scalar unit is meant for computations which are constant among all SIMD lanes. Using the scalar unit instead of SIMD units is a lot more efficient in memory and energy because we only need to store data once instead of 64 times, and we only need to execute operations only once instead of 64 times. Data values which are the same for a whole SIMD unit are called \emph{uniform}, in contrast to \emph{non-uniform} values which are different in each lane. The scalar unit is also responsible for managing the control flow of the SIMD unit. The vectorized instructions do not support jumps because a jump cannot be executed on a subset of lanes. Instead, jump instructions are executed on the scalar unit. We will come back to that later, with a more detailed explanation of branching.

Another unit on a \gls{gcn} GPU is the instruction scheduler. It is responsible for fetching new instructions and handing them to the responsible unit, e.g. the scalar unit or the SIMD unit. For our current setup with one SIMD unit, one scalar unit and one instruction scheduler, the scalar unit will only get an instruction every 4 cycles because it works in synchronization with the SIMD unit and the instruction scheduler will only output instructions every 4\textsuperscript{th} cycle. The only unit which is occupied the whole time is the vector unit. For this reason, every scalar unit and instruction scheduler is responsible for 4 SIMD units. A pack of 4 SIMD units, one scalar unit and one instruction scheduler is called \gls{cu}. Additionally, one \gls{cu} contains some local memory called \gls{lds}. One GPU consists of multiple such packs, for example a recent AMD Radeon VII GPU contains 60 \glspl{cu}. For a floating-point instruction such as an addition, which takes 4 cycles, this accumulates to a total of $60 \cdot 4 = 240$ SIMD units and a maximum of $240 \cdot 64 = 15360$ SIMD computations running in parallel. Every cycle, $240 \cdot 16 = 3840$ will get ready. And as a computation takes 4 cycles, 15360 are currently \enquote{in-flight}.

As emphasized before, having divergent control flow is not simply possible for program instances which get executed on one SIMD unit. For this purpose, AMD \gls{gcn} GPUs have an \emph{EXEC mask}, which contains one bit for every lane of a SIMD unit. If this bit is one, it means a lane is active, if it is zero, a lane is inactive and all SIMD operations are a no-op for this lane. When the program encounters a branch instruction, there are several possibilities. If all lanes take one branch, the scalar unit can execute a jump and execution continues as normal. If a part of the lanes take one branch and the rest takes another branch, both branches have to be executed. Typically, the execution mask for the first branch is set and the first branch is executed. Then the execution mask is flipped and the second branch is executed. After both branches are executed, the execution mask is reset to all ones again.

\paragraph{Software} This section explains how developers run code on GPUs. Developers which write code for GPUs do not have to handle all details of the hardware. Similar to CPUs where the operating system initializes the processors when starting and handles the resource allocations of processes, GPUs are managed by a \emph{driver}. The driver abstracts from the concrete underlying hardware and gives developers an interface that is easier to use.

Similar to different operating systems and standards like POSIX and the Windows \gls{api}, there exist different interfaces to communicate with graphic drivers. Two open standards, which are implemented by multiple vendors, are OpenGL and Vulkan. A specialty in comparison to CPUs is that the compiler is integrated into the driver in many APIs. For example in OpenGL this would be accomplished with the \texttt{glShaderSource} and \texttt{glCompileShader} function. In Vulkan there is \texttt{vkCreateShaderModule} for this purpose.

In this section we explain the structure of compiling and running shaders with the AMDVLK Vulkan driver on Linux. This driver is based on the LLVM compiler framework. Typically, shaders for Vulkan are written in the \gls{glsl}. The developer first has to translate this source coded to the intermediate language \emph{SPIR-V}. The resulting SPIR-V code can be passed to the Vulkan API. From there is passed to the LLPC (the llvm pipeline compiler). The SPIR-V code gets then converted into the LLVM \gls{ir}. After some intermediate transformations on this \gls{ir}, the LLVM compiler gets called. In this part most optimizations happens.
The LLVM framework also converts the IR to another IR, the \emph{SelectionDAG} -- as the name says a directed, acyclic graph of instructions~\cite{llvmSelectionDag}. The SelectionDAG gets linearized and transformed into \emph{MachineIR}~\cite{llvmSelectionDag}. MachineIR is the intermediate representation which is most similar to the native \gls{isa} of the graphics card. The conversion to the \gls{isa} happens in the last step. The output of LLVM is a file in the \gls{elf}. The LLPC applies some patches to the generated \gls{elf} file and then returns it.

When running a shader, the compiled \gls{elf} file is passed to the \gls{pal}. This library is responsible for talking to the operating system and the in-kernel part of the driver also called \gls{kmd}. It reads meta-information from the ELF file and uploads the compiled code to the GPU to execute it. The stages to run code on a GPU thus look like this:

\begin{enumerate}
	\item Developer writes human readable \gls{glsl} code
	\item \texttt{glslangValidator} converts \gls{glsl} to SPIR-V bytecode
	\item Developer passes bytecode to Vulkan driver with \texttt{vkCreateShaderModule}
	\item LLPC converts SPIR-V to LLVM IR code
	\item LLPC transforms the LLVM IR to make it valid for the LLVM framework
	\item LLVM optimizes the IR and converts it to the SelectionDAG
	\item LLVM transforms the SelectionDAG to MachineIR
	\item LLVM converts MachineIR to GPU ISA and returns an ELF file
\end{enumerate}
The compilation is done now, the next steps are loading and executing the code.
\begin{enumerate}
	\setcounter{enumi}{8}
	\item PAL reads the ELF file and loads it onto the GPU
	\item The GPU runs the code
\end{enumerate}

\subsection{PGO in LLVM}
\label{sub:pgo-background}
The LLVM compiler framework has existing infrastructure for profile-guided optimization. This infrastructure can be used by all frontends to LLVM like the C/C++ compiler \texttt{clang} or the Rust compiler. LLVM supports two different profiling techniques, a sampling profiler or instrumentation. We will concentrate on the instrumenting profiler in the following.

\subsection{Basic Block Counting}
\label{sub:counter-instrumentation}
One of the instrumentations inserts code to count how often each basic block is executed. The simplest version of such an instrumentation would insert one counter at each block. Let us consider an example with an if-else-block. The instrumentation would insert four counters: Before the branching, in the if-block, in the else-block and after the branching in the following block. However, we can get the same information with only two counters. We can count the executions of the if-part and the total executions either in the beginning or in the end. To get the counter of the else-part we subtract the if-part counter from the total amount.

In 1973, \citet{Knuth1973} showed and proofed an algorithm to find a minimal set of blocks where we need to insert counters. This algorithm is used in LLVM.

\subsection{The Executable and Linkable Format}
\label{sub:elf}
% TODO
The \gls{elf} is a file format used mainly on various Unix systems. On Linux, programs, libraries and also object files created by compilers are encoded in \gls{elf}.

High-level overview of relocations~\cite{BenderskyRelocations}
\texttt{sh\_link} specifies symbol section, \texttt{sh\_info} the section to apply the relocations to. The \texttt{sh} means \emph{Section Header}.

The modern way is \gls{pic} because it allows sharing library code between processes in memory~\cite{BenderskyPic}. Advantages of PIC are currently not relevant for GPUs as there are no shared libraries that have to be linked. But it takes one more dereference.