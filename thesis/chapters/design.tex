\clearpage
\bigsection{Design}
The LLVM project, their compiler framework and their C compiler clang successfully use \gls{bb} counters for profile-guided optimizations on CPUs. Therefore BB counters suggest themselves to be used for PGO on GPUs.

Where to integrate that instrumentation: In the llpc patcher

How does it work with pal?
How do we bring the information back to the cpu and write them to disk?

Implementation:

How to call it: command line option or environment variable (with shader hash?)

\subsection{Workflow}
\label{sub:workflow}
\Gls{pgo} is a common technique for applications running on CPUs.
Developers are already used to the way PGO is applied.
When using a compiler that builds upon the LLVM framework, the workflow is the following:
\begin{enumerate}
	\item The application is compiled with profiling instrumentation
	\item The application is run $n$ times and produces an output file on each run
	\item The $n$ files are merged into a single file which contains all profiling information
	\item The application is compiled again and optimized using the data from the generated file
\end{enumerate}
As the AMD Vulkan driver is based on LLVM and developers already know this workflow, it suggests itself to use the same workflow for PGO on GPUs.
When using PGO with \texttt{clang}, we use command line arguments to tell LLVM which options we want to use. When invoking the compiler through the Vulkan API, we cannot use command line arguments.
To keep the usage and implementation of options simple, we use environment variables to pass filenames and other options.

\subsection{Storing Data}
\label{sub:save-design}
% TODO
LLVM generates code for writing the collected data into a file and calls this code from a destructor.
Destructors are a list of functions which are called when an application exits.
The code creates a file containing all the profile data, consisting of counters, metadata and possible variable values if value profiling is enabled.
GPUs do not support the destructor list and they are unable to access the file system of the host.
So directly creating and writing the result from the GPU is impossible.

We try to reuse the file writing code from LLVM but inside the driver instead of inside the shader.
The driver has access to all the data generated by the shader, it can map the memory to the CPU and read it.

\subsection{Basic Block Counting}
\label{sub:design_counters}
In LLVM, the counters are used mostly for linearization. To get the accurate information at this stage in the compiler, the \gls{cfg} is instrumented after most optimizations, when it will not be changed significantly anymore. An earlier instrumentation can not give us the needed information when basic blocks get duplicated because we would count them as the same block. An early instrumentation can also harm following optimizations, e.g. by making it more difficult to merge duplicated code when different counters get incremented.

After the optimizations, the \gls{cfg} changes one more time on GPUs using the \gls{simt} model: The CFG structurization converts the control flow so it sets the EXEC mask and runs both, the if- and the else-branch if necessary. We can insert the instrumentation counters either before the CFG structurization or afterwards and count whole SIMD units. When comparing both options, we need to take into account, that the whole SIMD unit always executes the same instruction. For example if only a single lane executes a BB, e.g. an operation that uses aggregated values from the whole unit, we will get a low counter for this block compared to the rest of the blocks. Nevertheless, it does not make sense to deprioritize the block in the linearization step because the whole unit executes the instruction, just with all lanes masked out but one. This is a strong argument in favor of inserting the BB counters per unit instead of per lane. For this work, we implemented and compare both methods.

\subsection{Uniform Variables}
\label{sub:uniformity}
Uniformity plays an important role for the efficiency of GPUs~\cite{Chen2016}. Uniform operations need to be executed only once per SIMD unit instead of 64 times and they can be run in parallel to other instructions that are executed per lane. These possible optimizations make it desirable to find out which variables are uniform and which divergent. LLVM contains a static divergence analysis for this purpose which uses known sources of divergence like shader input variables and marks all possibly divergent variables. All non-marked variables are known to be uniform.

However, this analysis may not find all uniform variables. Some variables may always be uniform at runtime but the compiler does not have enough knowledge about the application logic to prove that it is always the case. Additionally, it may be interesting to know variables which are uniform in \emph{most} cases. If we know variables that are mostly uniform, we can introduce specialized fast paths and fall back to a slower, non-uniform version if necessary.
% TODO ^ Put that into possible optimizations

% TODO
The first idea on how to detect uniform branches is simple. If we count the executed basic blocks per SIMD unit and we have a branch where either an if-block or an else-block gets executed, we can compute how often both blocks are executed so we know how uniform the condition is.
However, this technique does not work if we have no else-block. The uniformity of other variables, e.g. memory addresses may also be interesting, so we decided to implement a generic analysis that finds out how often a variable is uniform.
