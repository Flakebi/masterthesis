\clearpage
\bigsection{Design}
The LLVM project, their compiler framework and their C compiler clang successfully use \gls{bb} counters for profile-guided optimizations on CPUs. Therefore BB counters suggest themselves to be used for PGO on GPUs.

Where to integrate that instrumentation: In the llpc patcher

How does it work with pal?
How do we bring the information back to the cpu and write them to disk?

Implementation:

How to call it: command line option or environment variable (with shader hash?)

\subsection{Workflow}
\label{sub:workflow}
\Gls{pgo} is a common technique for applications running on CPUs.
Developers are already used to the way PGO is applied.
When using a compiler that builds upon the LLVM framework, the workflow is the following:
\begin{enumerate}
	\item The application is compiled with profiling instrumentation
	\item The application is run $n$ times and produces an output file on each run
	\item The $n$ files are merged into a single file which contains all profiling information
	\item The application is compiled again and optimized using the data from the generated file
\end{enumerate}
As the AMD Vulkan driver is based on LLVM and developers already know this workflow, it suggests itself to use the same workflow for PGO on GPUs.
When using PGO with \texttt{clang}, we use command line arguments to tell LLVM which options we want to use. When invoking the compiler through the Vulkan API, we cannot use command line arguments.
To keep the usage and implementation of options simple, we use environment variables to pass filenames and other options.

\subsection{Storing Data}
\label{sub:save-design}
On CPUs, LLVM generates code for writing the collected data into a file.
Most of the file writing code is implemented in a static library which gets linked in when compiling a program with PGO instrumentation.
The code creates a file containing all the profile data, consisting of counters, metadata and variable values if value profiling is enabled.
To call the generated code when the application exits, LLVM uses a global destructor.
The destructors are a list of functions that are called when an application quits.

GPUs currently do not support the destructor list. Additionally, shader code is unable to access the file system of the host.
So directly creating and writing result files from the GPU is impossible, we have to use the driver on the CPU for this task.

We want to reuse the file writing code from LLVM but inside the driver instead of inside the shader.
Therefore we link the static library of LLVM into the driver.
Before calling the dump file function, we map the counters in GPU memory to the CPU and we tell LLVM the name of the file we want to write.
The filename is set by an environment variable and it can contain the pipeline id so the data does not get overwritten when saving multiple pipelines.

\subsection{Basic Block Counting}
\label{sub:design_counters}
In LLVM, the counters are used mostly for linearization. To get the accurate information at this stage in the compiler, the \gls{cfg} is instrumented after most optimizations, when it will not be changed significantly anymore.
An earlier instrumentation can not give us the needed information when basic blocks get duplicated because we would count them as the same block. An early instrumentation can also harm following optimizations, e.g. by making it more difficult to merge duplicated code when different counters get incremented.

After the optimizations, the \gls{cfg} changes one more time on GPUs using the \gls{simt} model: The CFG structurization that makes it possible to run shader code on SIMD units.
We can insert the instrumentation counters either before the CFG structurization or afterwards and count whole SIMD units. When comparing both options, we need to take into account, that the whole SIMD unit always executes the same instruction.
For example if only a single lane executes a BB, e.g. an operation that uses aggregated values from the whole unit, we will get a low counter for this block compared to the rest of the blocks. Nevertheless, it does not make sense to deprioritize the block in the linearization step because the whole unit executes the instruction, just with all lanes masked out but one.
This is a strong argument in favor of inserting the BB counters per unit instead of per lane. For this work, we implemented and compare both methods.

After capturing counters, the next task is to use them. As LLVM is already using basic block counters for linearization if they are available, this is a low-hanging fruit.
The amount of registers that a shader needs can limit the occupancy on a compute unit, i.e. the level of multithreading, which is important to hide memory latency.
If the occupancy is too low, we cannot hide memory latency good enough and the GPU will be waiting a significant amount of time.
To improve that situation, we can spill some registers to (scratch) memory and increase the occupancy in that way.
Basic block counters let us decide which parts of the code are executed seldom or not at all, so we can spill registers there.
To estimate the possible performance improvement without fully implementing this optimization, we remove all parts of the shader code that are never executed and measure the impact on used registers and performance.
We cannot use this strategy in a production driver because code that is not executed in a PGO benchmark scene is not necessarily dead code. However, this test can provide us valuable information if such an optimization can be worth the effort.

\subsection{Uniform Variables}
\label{sub:uniformity}
Uniformity plays an important role for the efficiency of GPUs~\cite{Chen2016}. Uniform operations need to be executed only once per SIMD unit instead of 64 times and they can be run in parallel to other instructions that are executed per lane. These possible optimizations make it desirable to find out which variables are uniform and which divergent. LLVM contains a static divergence analysis for this purpose which uses known sources of divergence like shader input variables and marks all possibly divergent variables. All non-marked variables are known to be uniform.

However, this analysis may not find all uniform variables. Some variables may always be uniform at runtime but the compiler does not have enough knowledge about the application logic to prove that it is always the case. Additionally, it may be interesting to know variables which are uniform in \emph{most} cases. If we know variables that are mostly uniform, we can introduce specialized fast paths and fall back to a slower, non-uniform version if necessary.

The first idea to detect uniform branches is simple. At an if-else, we count the executed basic blocks per SIMD unit, so afterwards we know how often the unit ran the if-block and the else-block. From these numbers we can compute how often both blocks are executed in one run.
If both blocks are executed, we know that the condition is not uniform, otherwise either the else-block or the if-block would have been executed, not both. So we can compute how often a condition is uniform or not.
However, this technique does only work if we have an else-block. Without else-block we cannot compute the uniformity of a condition. In addition to that problem, the uniformity of other variables, e.g. memory addresses is also interesting.
For these reasons we decided to implement a generic analysis that finds out how often a variable is uniform.
